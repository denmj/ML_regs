{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PreClf, MultiPreClf\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# MLP from sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# MNIST \n",
    "X_, y_ = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\")\n",
    "# Preprocessing normalisation \n",
    "X_ = X_ / 255\n",
    "\n",
    "y_ = y_.astype(int)\n",
    "\n",
    "# On-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = encoder.fit_transform(y_.reshape(-1, 1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56000, 784)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn y_train from one-hot to integer\n",
    "y_train = np.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_new(object):\n",
    "    def __init__(self, alpha=0.01, n_iterations=50, input_layer_size=784, hidden_layers_size=[128, 64],\n",
    "                 n_outputs=10, regularization='l2', lambda_reg=0.01):\n",
    "        \n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        \n",
    "        self.eta = alpha\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.n_iterations = n_iterations\n",
    "        self.input_layer_size = input_layer_size\n",
    "        self.hidden_layers_size = hidden_layers_size\n",
    "        self.n_outputs = n_outputs\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        self.layer_sizes = [self.input_layer_size] + self.hidden_layers_size + [self.n_outputs]\n",
    "\n",
    "    def init_weights(self, he=True):\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            if he:\n",
    "                he_std_dev = np.sqrt(2 / self.layer_sizes[i])\n",
    "                self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * he_std_dev)\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) - 0.5)\n",
    "            self.bias.append(np.random.rand(1, self.layer_sizes[i + 1]) - 0.5)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            linear_output = np.dot(activations[i], self.weights[i]) + self.bias[i]\n",
    "            activation_output = self.relu(linear_output)\n",
    "            activations.append(activation_output)\n",
    "        last_linear_output = np.dot(activations[-1], self.weights[-1]) + self.bias[-1]\n",
    "        if self.n_outputs == 1:\n",
    "            last_activation_output = self.sigmoid(last_linear_output)\n",
    "        else:\n",
    "            last_activation_output = self.softmax(last_linear_output)\n",
    "        activations.append(last_activation_output)\n",
    "        return activations\n",
    "\n",
    "    def backward_propagation(self, X, Y, activations):\n",
    "\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dZ3 = activations[-1] - Y\n",
    "        dW3 = 1 / m * np.dot(activations[-2].T, dZ3)\n",
    "        db3 = 1 / m * np.sum(dZ3, axis=0, keepdims=True)\n",
    "        dZ2 = np.dot(dZ3, self.weights[-1].T) * self.relu_derivative(activations[-2])\n",
    "        dW2 = 1 / m * np.dot(activations[-3].T, dZ2)\n",
    "        db2 = 1 / m * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        dZ1 = np.dot(dZ2, self.weights[-2].T) * self.relu_derivative(activations[-3])\n",
    "        dW1 = 1 / m * np.dot(activations[0].T, dZ1)\n",
    "        db1 = 1 / m * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        return dW1, dW2, dW3, db1, db2, db3\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
    "    \n",
    "    def accuracy(self, Y_pred, Y):\n",
    "        pred = np.argmax(Y_pred, axis=1)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        return np.mean(pred == Y)\n",
    "    \n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(z, 0)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        Z -= np.max(Z, axis=0, keepdims=True)  # Improve numerical stability\n",
    "        expZ = np.exp(Z)\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "    \n",
    "\n",
    "    def update_parameters(self, dW1, dW2, dW3, db1, db2, db3):\n",
    "\n",
    "        self.weights[0] -= self.eta * dW1\n",
    "        self.weights[1] -= self.eta * dW2\n",
    "        self.weights[2] -= self.eta * dW3\n",
    "        self.bias[0] -= self.eta * db1\n",
    "        self.bias[1] -= self.eta * db2\n",
    "        self.bias[2] -= self.eta * db3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        self.init_weights()\n",
    "        for i in range(self.n_iterations):\n",
    "            activations = self.forward_propagation(X)\n",
    "            loss = self.compute_loss(activations[-1], Y)\n",
    "            dW1, dW2, dW3, db1, db2, db3 = self.backward_propagation(X, Y, activations)\n",
    "            self.update_parameters(dW1, dW2, dW3, db1, db2, db3)\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Loss at iteration {i}: {loss}\")\n",
    "                print(f\"Accuracy at iteration {i}: {self.accuracy(activations[-1], Y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, alpha=0.01, n_iterations=50, input_layer_size=784, hidden_layers_size=[128, 64],\n",
    "                 n_outputs=10, regularization='l2', lambda_reg=0.01):\n",
    "        \n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        \n",
    "        self.eta = alpha\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.n_iterations = n_iterations\n",
    "        self.input_layer_size = input_layer_size\n",
    "        self.hidden_layers_size = hidden_layers_size\n",
    "        self.n_outputs = n_outputs\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        self.layer_sizes = [self.input_layer_size] + self.hidden_layers_size + [self.n_outputs]\n",
    "\n",
    "    def init_weights(self, he=True):\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            if he:\n",
    "                he_std_dev = np.sqrt(2 / self.layer_sizes[i])\n",
    "                self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * he_std_dev)\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) - 0.5)\n",
    "            self.bias.append(np.random.rand(1, self.layer_sizes[i + 1]) - 0.5)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            linear_output = self.weights[i].dot(activations[i]) + self.bias[i]\n",
    "            activation_output = self.relu(linear_output)\n",
    "            activations.append(activation_output)\n",
    "        last_linear_output = self.weights[-1].dot(activations[-1]) + self.bias[-1]\n",
    "        if self.n_outputs == 1:\n",
    "            last_activation_output = self.sigmoid(last_linear_output)\n",
    "        else:\n",
    "            last_activation_output = self.softmax(last_linear_output)\n",
    "        activations.append(last_activation_output)\n",
    "        return activations\n",
    "\n",
    "    def backpropagation(self, X, y, activations):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        w_grads = [np.zeros_like(w) for w in self.weights]\n",
    "        b_grads = [np.zeros_like(b) for b in self.bias]\n",
    "\n",
    "        if self.n_outputs == 1:\n",
    "            error = activations[-1] - y  \n",
    "        else:\n",
    "            error = (activations[-1] - y) / activations[-1] * (1 - activations[-1])\n",
    "        \n",
    "        for layer in reversed(range(len(self.weights))):\n",
    "            w_grads[layer] = np.dot(activations[layer].T, error) / n_samples\n",
    "            b_grads[layer] = np.sum(error, axis=0, keepdims=True) / n_samples\n",
    "            if layer > 0:\n",
    "                error = np.dot(error, self.weights[layer].T) * self.relu_prime(activations[layer])\n",
    "\n",
    "        if self.regularization == 'l2':\n",
    "            w_grads[layer] += (self.lambda_reg / n_samples) * self.weights[layer]\n",
    "        elif self.regularization == 'l1':\n",
    "            w_grads[layer] += (self.lambda_reg / n_samples) * np.sign(self.weights[layer])\n",
    "        \n",
    "        # Update weights and biases\n",
    "        for layer in range(len(self.weights)):\n",
    "            self.weights[layer] -= self.eta * w_grads[layer]\n",
    "            self.bias[layer] -= self.eta * b_grads[layer]\n",
    "\n",
    "        return error\n",
    "\n",
    "    def back_propagation(self, X, y, activations):\n",
    "\n",
    "        error = None\n",
    "        for layer in range(1, len(self.weights) + 1):\n",
    "            if layer == len(self.weights):\n",
    "                error = activations[-1] - y\n",
    "            else:\n",
    "                if error is not None:\n",
    "                    error = np.dot(error, self.weights[-layer + 1].T) * self.relu_prime(activations[-layer])\n",
    "                else:\n",
    "                    # Handle case where error is not yet defined\n",
    "                    error = np.zeros_like(activations[-layer])\n",
    "\n",
    "            if self.regularization == 'l2':\n",
    "                reg_penalty = self.lambda_reg * self.weights[-layer]\n",
    "            elif self.regularization == 'l1':\n",
    "                reg_penalty = self.lambda_reg * np.sign(self.weights[-layer])\n",
    "            else:\n",
    "                reg_penalty = 0\n",
    "            delta = error * self.eta\n",
    "            self.weights[-layer] -= (np.dot(activations[-layer - 1].T, delta) + reg_penalty)\n",
    "            self.bias[-layer] -= np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, X_val=None, y_val=None, batch_size=20, verbose=True):\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        for i in range(self.n_iterations):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            batch_losses = []\n",
    "\n",
    "            for j in range(0, X.shape[0], batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[j:j + batch_size]\n",
    "                y_batch = y_shuffled[j:j + batch_size]\n",
    "\n",
    "                activations = self.forward_propagation(X_batch)\n",
    "                batch_loss = self.cross_entropy_loss(y_batch, activations[-1])\n",
    "                batch_losses.append(batch_loss)\n",
    "\n",
    "                self.backpropagation(X_batch, y_batch, activations)\n",
    "\n",
    "            epoch_loss = np.mean(batch_losses)\n",
    "            pred = self.predict(X)\n",
    "            acc = self.accuracy(np.argmax(y, axis=1), pred)\n",
    "            if verbose: \n",
    "                print(f'Epoch: {i}, Training loss : {epoch_loss}')\n",
    "                print(f'Epoch: {i}, Training accuracy : {acc}')\n",
    "\n",
    "            training_loss.append(epoch_loss)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_activations = self.forward_propagation(X_val)\n",
    "                val_loss = self.cross_entropy_loss(y_val, val_activations[-1])\n",
    "                validation_loss.append(val_loss)\n",
    "                if verbose: \n",
    "                    print(f'Epoch: {i}, Validation loss: {val_loss}')\n",
    "\n",
    "        return training_loss, validation_loss\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self, z):\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    def relu_prime(self, z):\n",
    "        return (z > 0).astype(int)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        A = np.exp(z) / sum(np.exp(z))\n",
    "        return A\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        if y_true.shape[1] == 1:\n",
    "            return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "        else:\n",
    "            return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        return np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions =  self.forward_propagation(X)[-1]\n",
    "        return np.argmax(predictions, axis=1) if self.n_outputs > 1 else (predictions > 0.5).astype(int)\n",
    "\n",
    "    def simple_fwd(self,W1, b1, W2, b2, X):\n",
    "        \n",
    "        Z1 = W1.dot(X) + b1\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = W2.dot(A1) + b2\n",
    "        A2 = self.softmax(Z2)\n",
    "\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def simple_bckprop(self,  Z1, A1, Z2, A2, W1, W2, X, y):\n",
    "            \n",
    "        dZ2 = A2 - y\n",
    "        dW2 = 1 / X.shape[0] * dZ2.dot(A1.T)\n",
    "        db2 = 1 / X.shape[0] * np.sum(dZ2)\n",
    "        dZ1 = W2.T.dot(dZ2) * self.relu_prime(Z1)\n",
    "        dW1 = 1 / X.shape[0] * dZ1.dot(X.T)\n",
    "        db1 = 1 / X.shape[0] * np.sum(dZ1)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_params(self, dW1, db1, dW2, db2, alpha):\n",
    "\n",
    "        self.weights[0] -= alpha * dW1\n",
    "        self.bias[0] -= alpha * db1\n",
    "        self.weights[1] -= alpha * dW2\n",
    "        self.bias[1] -= alpha * db2\n",
    "\n",
    "        return self.weights[0], self.bias[0], self.weights[1], self.bias[1]\n",
    "\n",
    "    def train_two(self, X, y, alpha, iter):\n",
    "\n",
    "        W1,  W2 = self.weights\n",
    "        b1, b2 = self.bias\n",
    "\n",
    "        for i in range(iter):\n",
    "            Z1, A1, Z2, A2 = self.simple_fwd(W1, b1, W2, b2, X)\n",
    "\n",
    "            dW1, db1, dW2, db2 = self.simple_bckprop(Z1, A1, Z2, A2, W1, W2, X, y)\n",
    "\n",
    "            W1, b1, W2, b2 = self.update_params(dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                # print(f'Iteration: {i}, Training loss : {self.cross_entropy_loss(y, A2)}')\n",
    "                print(f'Iteration: {i}, Training accuracy : {self.accuracy(np.argmax(y, axis=1), np.argmax(A2, axis=1))}')\n",
    "\n",
    "        return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_main = MLP_new(alpha=0.01, n_iterations=500, \n",
    "                            input_layer_size=784, hidden_layers_size=[20, 10], \n",
    "                            n_outputs=10,\n",
    "                            lambda_reg=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_main.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model_main.forward_propagation(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 10)\n",
      "(10, 10)\n",
      "(1, 10)\n",
      "(56000, 10)\n",
      "(20, 10)\n",
      "(1, 10)\n",
      "(56000, 20)\n",
      "(784, 20)\n",
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "dW1, dW2, dW3, db1, db2, db3 = model_main.backward_propagation(X_train, y_train, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_main.update_parameters(dW1, dW2, dW3, db1, db2, db3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model_main.accuracy(a[-1], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1584107142857143"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 10.933050971285224\n",
      "Accuracy at iteration 0: 0.09928571428571428\n",
      "Loss at iteration 10: 10.933050971285224\n",
      "Accuracy at iteration 10: 0.09928571428571428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[552], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_main\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[546], line 99\u001b[0m, in \u001b[0;36mMLP_new.train\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_weights()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iterations):\n\u001b[1;32m---> 99\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], Y)\n\u001b[0;32m    101\u001b[0m     dW1, dW2, dW3, db1, db2, db3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_propagation(X, Y, activations)\n",
      "Cell \u001b[1;32mIn[546], line 30\u001b[0m, in \u001b[0;36mMLP_new.forward_propagation\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     28\u001b[0m activations \u001b[38;5;241m=\u001b[39m [X]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[i]\n\u001b[0;32m     31\u001b[0m     activation_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(linear_output)\n\u001b[0;32m     32\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(activation_output)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_main.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
