{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance, ImageFilter\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "sys.path.append('../')  \n",
    "\n",
    "from Models.alexnet import AlexNet\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd() + '\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeEnhancement:\n",
    "    def __call__(self, img):\n",
    "        return img.filter(ImageFilter.FIND_EDGES)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize(256),  # Resize slightly larger than final size\n",
    "    transforms.RandomResizedCrop(224),  # Random crop back down to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
    "    transforms.RandomRotation(15),  # Rotate by +/- 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, ),  # Randomly change brightness and contrast\n",
    "    # transforms.Resize(224),  # Resize to 224x224 to match AlexNet input size\n",
    "    # transforms.Lambda(lambda img: EdgeEnhancement()(img)),\n",
    "    transforms.ToTensor(),   # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DropletDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load images and labels\n",
    "        for label in ['background', 'droplets']:\n",
    "            class_dir = os.path.join(data_dir, label)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith('.jpg'):  # Modify if needed for different extensions\n",
    "                    img_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(1 if label == 'droplets' else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB if not already\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "droplet_dataset = DropletDataset(data_dir=base_path, transform=transform)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet_droplet = AlexNet(num_classes=10, channels=1).to(device)\n",
    "\n",
    "alexnet_droplet.load_state_dict(torch.load('alexnet_model_mnist_full.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AlexNet                                  [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 256, 6, 6]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 55, 55]           7,808\n",
       "│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n",
       "│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n",
       "│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n",
       "│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n",
       "│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n",
       "│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n",
       "│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n",
       "│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n",
       "│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n",
       "│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n",
       "├─Sequential: 1-3                        [1, 10]                   --\n",
       "│    └─Dropout: 2-14                     [1, 9216]                 --\n",
       "│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n",
       "│    └─ReLU: 2-16                        [1, 4096]                 --\n",
       "│    └─Dropout: 2-17                     [1, 4096]                 --\n",
       "│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n",
       "│    └─ReLU: 2-19                        [1, 4096]                 --\n",
       "│    └─Linear: 2-20                      [1, 10]                   40,970\n",
       "==========================================================================================\n",
       "Total params: 57,029,322\n",
       "Trainable params: 57,029,322\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 663.78\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 3.95\n",
       "Params size (MB): 228.12\n",
       "Estimated Total Size (MB): 232.26\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(alexnet_droplet, input_size=(1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the last layer of the classifier to output 2 classes instead of 10\n",
    "alexnet_droplet.classifier[6] = nn.Linear(4096, 2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_adam = optim.Adam(alexnet_droplet.parameters(), lr=1e-4)\n",
    "optimizer_sgd = torch.optim.SGD(alexnet_droplet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
