{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance, ImageFilter\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "# pretrained models (AlexNet) from torchvision\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "sys.path.append('../')  \n",
    "\n",
    "from Models.alexnet import AlexNet\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd() + '\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeEnhancement:\n",
    "    def __call__(self, img):\n",
    "        return img.filter(ImageFilter.FIND_EDGES)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize(256),  # Resize slightly larger than final size\n",
    "    transforms.RandomResizedCrop(224),  # Random crop back down to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
    "    transforms.RandomRotation(15),  # Rotate by +/- 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, ),  # Randomly change brightness and contrast\n",
    "    # transforms.Resize(224),  # Resize to 224x224 to match AlexNet input size\n",
    "    # transforms.Lambda(lambda img: EdgeEnhancement()(img)),\n",
    "    transforms.ToTensor(),   # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DropletDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load images and labels\n",
    "        for label in ['background', 'droplets']:\n",
    "            class_dir = os.path.join(data_dir, label)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith('.jpg'):  # Modify if needed for different extensions\n",
    "                    img_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(1 if label == 'droplets' else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB if not already\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = torch.tensor(label)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "droplet_dataset = DropletDataset(data_dir=base_path, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(droplet_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get alexnet pretrained on ImageNet\n",
    "alexnet_droplet_v2 = models.alexnet(weights=True)\n",
    "\n",
    "\n",
    "alexnet_droplet = AlexNet(num_classes=10, channels=1).to(device)\n",
    "\n",
    "alexnet_droplet.load_state_dict(torch.load('alexnet_model_mnist_full.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(alexnet_droplet, input_size=(1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(alexnet_droplet_v2, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the last layer of the classifier to output 2 classes instead of 10\n",
    "alexnet_droplet.classifier[6] = nn.Linear(4096, 2).to(device)\n",
    "\n",
    "alexnet_droplet_v2.classifier[6] = torch.nn.Linear(alexnet_droplet_v2.classifier[6].in_features, 2)\n",
    "\n",
    "# image is also with 1 channel\n",
    "alexnet_droplet_v2.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "alexnet_droplet_v2 = alexnet_droplet_v2.to(device)\n",
    "\n",
    "\n",
    "# accuracy = Accuracy(task='multiclass', num_classes=2)\n",
    "\n",
    "accuracy = Accuracy(task='multiclass', num_classes=2)\n",
    "\n",
    "accuracy = accuracy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(alexnet_droplet_v2, input_size=(1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([2.0, 1.0])  # Adjust these values based on your understanding of class importance\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "optimizer_adam = optim.Adam(alexnet_droplet.parameters(), lr=1e-1)\n",
    "optimizer_sgd = torch.optim.SGD(alexnet_droplet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "# optim adam for alexnet_v2\n",
    "optimizer_adam_v2 = optim.Adam(alexnet_droplet_v2.parameters(), lr=1e-4)\n",
    "optimizer_sgd_v2 = torch.optim.SGD(alexnet_droplet_v2.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler for learning rate decay\n",
    "scheduler_adam = StepLR(optimizer_adam, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "scheduler_adam_v2 = StepLR(optimizer_adam_v2, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([1, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet on MNIST10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 305747.1719, Accuracy: 0.5600\n",
      "Epoch 2/40, Loss: 1486398.4133, Accuracy: 0.5267\n",
      "Epoch 3/40, Loss: 66583.0197, Accuracy: 0.5222\n",
      "Epoch 4/40, Loss: 64859.9824, Accuracy: 0.5033\n",
      "Epoch 5/40, Loss: 10.0272, Accuracy: 0.4880\n",
      "Epoch 6/40, Loss: 91058.9718, Accuracy: 0.4967\n",
      "Epoch 7/40, Loss: 1597.8118, Accuracy: 0.4924\n",
      "Epoch 8/40, Loss: 139.8350, Accuracy: 0.5133\n",
      "Epoch 9/40, Loss: 245712.2717, Accuracy: 0.5007\n",
      "Epoch 10/40, Loss: 102.0761, Accuracy: 0.4940\n",
      "Epoch 11/40, Loss: 0.2803, Accuracy: 0.5018\n",
      "Epoch 12/40, Loss: 0.2913, Accuracy: 0.4978\n",
      "Epoch 13/40, Loss: 0.2723, Accuracy: 0.4933\n",
      "Epoch 14/40, Loss: 0.2205, Accuracy: 0.4933\n",
      "Epoch 15/40, Loss: 0.1823, Accuracy: 0.4920\n",
      "Epoch 16/40, Loss: 0.2116, Accuracy: 0.4900\n",
      "Epoch 17/40, Loss: 0.1824, Accuracy: 0.4906\n",
      "Epoch 18/40, Loss: 0.1917, Accuracy: 0.4852\n",
      "Epoch 19/40, Loss: 0.1235, Accuracy: 0.4895\n",
      "Epoch 20/40, Loss: 0.1554, Accuracy: 0.4857\n",
      "Epoch 21/40, Loss: 0.0908, Accuracy: 0.4917\n",
      "Epoch 22/40, Loss: 0.1364, Accuracy: 0.4942\n",
      "Epoch 23/40, Loss: 0.1482, Accuracy: 0.4951\n",
      "Epoch 24/40, Loss: 0.1092, Accuracy: 0.4975\n",
      "Epoch 25/40, Loss: 0.1608, Accuracy: 0.4928\n",
      "Epoch 26/40, Loss: 0.1070, Accuracy: 0.4931\n",
      "Epoch 27/40, Loss: 0.1214, Accuracy: 0.4923\n",
      "Epoch 28/40, Loss: 0.1543, Accuracy: 0.4936\n",
      "Epoch 29/40, Loss: 0.0990, Accuracy: 0.4961\n",
      "Epoch 30/40, Loss: 0.1204, Accuracy: 0.4980\n",
      "Epoch 31/40, Loss: 0.1270, Accuracy: 0.4972\n",
      "Epoch 32/40, Loss: 0.1104, Accuracy: 0.4977\n",
      "Epoch 33/40, Loss: 0.1147, Accuracy: 0.4974\n",
      "Epoch 34/40, Loss: 0.1342, Accuracy: 0.4965\n",
      "Epoch 35/40, Loss: 0.1194, Accuracy: 0.4964\n",
      "Epoch 36/40, Loss: 0.1206, Accuracy: 0.4948\n",
      "Epoch 37/40, Loss: 0.1425, Accuracy: 0.4939\n",
      "Epoch 38/40, Loss: 0.0988, Accuracy: 0.4968\n",
      "Epoch 39/40, Loss: 0.1017, Accuracy: 0.4978\n",
      "Epoch 40/40, Loss: 0.0998, Accuracy: 0.4980\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    alexnet_droplet.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_adam.zero_grad() # Zero the gradients\n",
    "\n",
    "        outputs = alexnet_droplet(images) # Forward pass\n",
    "\n",
    "        loss = criterion(outputs, labels) # Calculate the loss\n",
    "        accuracy.update(outputs, labels)\n",
    "\n",
    "        loss.backward() # Backward pass\n",
    "\n",
    "        optimizer_adam.step() # Update the weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler_adam.step() # Step the scheduler\n",
    "\n",
    "    train_loss /= len(droplet_dataset)\n",
    "    train_acc = accuracy.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.0246, Accuracy: 0.5000\n",
      "Epoch 2/40, Loss: 0.0236, Accuracy: 0.5500\n",
      "Epoch 3/40, Loss: 0.0228, Accuracy: 0.5444\n",
      "Epoch 4/40, Loss: 0.0222, Accuracy: 0.5783\n",
      "Epoch 5/40, Loss: 0.0210, Accuracy: 0.6040\n",
      "Epoch 6/40, Loss: 0.0212, Accuracy: 0.6000\n",
      "Epoch 7/40, Loss: 0.0217, Accuracy: 0.6076\n",
      "Epoch 8/40, Loss: 0.0217, Accuracy: 0.6075\n",
      "Epoch 9/40, Loss: 0.0214, Accuracy: 0.6007\n",
      "Epoch 10/40, Loss: 0.0214, Accuracy: 0.6067\n",
      "Epoch 11/40, Loss: 0.0195, Accuracy: 0.6139\n",
      "Epoch 12/40, Loss: 0.0180, Accuracy: 0.6233\n",
      "Epoch 13/40, Loss: 0.0208, Accuracy: 0.6297\n",
      "Epoch 14/40, Loss: 0.0187, Accuracy: 0.6310\n",
      "Epoch 15/40, Loss: 0.0180, Accuracy: 0.6396\n",
      "Epoch 16/40, Loss: 0.0167, Accuracy: 0.6450\n",
      "Epoch 17/40, Loss: 0.0155, Accuracy: 0.6533\n",
      "Epoch 18/40, Loss: 0.0175, Accuracy: 0.6559\n",
      "Epoch 19/40, Loss: 0.0166, Accuracy: 0.6625\n",
      "Epoch 20/40, Loss: 0.0146, Accuracy: 0.6670\n",
      "Epoch 21/40, Loss: 0.0150, Accuracy: 0.6721\n",
      "Epoch 22/40, Loss: 0.0132, Accuracy: 0.6791\n",
      "Epoch 23/40, Loss: 0.0121, Accuracy: 0.6849\n",
      "Epoch 24/40, Loss: 0.0120, Accuracy: 0.6908\n",
      "Epoch 25/40, Loss: 0.0142, Accuracy: 0.6952\n",
      "Epoch 26/40, Loss: 0.0143, Accuracy: 0.6995\n",
      "Epoch 27/40, Loss: 0.0165, Accuracy: 0.7000\n",
      "Epoch 28/40, Loss: 0.0141, Accuracy: 0.7019\n",
      "Epoch 29/40, Loss: 0.0116, Accuracy: 0.7067\n",
      "Epoch 30/40, Loss: 0.0146, Accuracy: 0.7098\n",
      "Epoch 31/40, Loss: 0.0106, Accuracy: 0.7144\n",
      "Epoch 32/40, Loss: 0.0176, Accuracy: 0.7163\n",
      "Epoch 33/40, Loss: 0.0135, Accuracy: 0.7180\n",
      "Epoch 34/40, Loss: 0.0134, Accuracy: 0.7208\n",
      "Epoch 35/40, Loss: 0.0130, Accuracy: 0.7230\n",
      "Epoch 36/40, Loss: 0.0135, Accuracy: 0.7241\n",
      "Epoch 37/40, Loss: 0.0148, Accuracy: 0.7245\n",
      "Epoch 38/40, Loss: 0.0132, Accuracy: 0.7260\n",
      "Epoch 39/40, Loss: 0.0156, Accuracy: 0.7274\n",
      "Epoch 40/40, Loss: 0.0122, Accuracy: 0.7295\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    alexnet_droplet_v2.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_adam_v2.zero_grad() # Zero the gradients\n",
    "\n",
    "        outputs = alexnet_droplet_v2(images) # Forward pass\n",
    "\n",
    "        loss = criterion(outputs, labels) # Calculate the loss\n",
    "        accuracy.update(outputs, labels)\n",
    "\n",
    "        loss.backward() # Backward pass\n",
    "\n",
    "        optimizer_adam_v2.step() # Update the weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler_adam.step() # Step the scheduler\n",
    "\n",
    "    train_loss /= len(droplet_dataset)\n",
    "    train_acc = accuracy.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Create a test scenario\n",
    "accuracy_ = Accuracy(task='multiclass', num_classes=2, average='macro')\n",
    "predictions = torch.tensor([[2.0, 1.0], [1.0, 3.0], [0.5, 0.2]])\n",
    "labels = torch.tensor([0, 1, 0])\n",
    "\n",
    "# Calculate accuracy\n",
    "print(accuracy_(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
