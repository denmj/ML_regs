{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import VOCDetection\n",
    "\n",
    "sys.path.append('../')  \n",
    "from Object_Detection import selective_search\n",
    "from Object_Detection.bbox import calculate_iou\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Settings \n",
    "\n",
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data\\VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "voc_dataset = VOCDetection(root='./data', \n",
    "                           year='2012', \n",
    "                           image_set='train',\n",
    "                           download=True, \n",
    "                        #    transform=transform\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and its finetuned weights (PASCAL VOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the ResNet50 model without pretrained weights\n",
    "resnet50 = models.resnet50(weights=False)\n",
    "\n",
    "resnet50.fc = torch.nn.Linear(resnet50.fc.in_features, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load your finetuned weights\n",
    "# Make sure that the path 'voc_finetune_resnet_weights.pth' is correct and accessible\n",
    "resnet50.load_state_dict(torch.load('resnet50_pascal_voc.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model=resnet50, input_size=(1, 3, 227, 227), col_width=20,\n",
    "                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Modified(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ResNet50Modified, self).__init__()\n",
    "        # Adapt this line if your model was changed differently\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-1])  # Typically removes the final fully connected layer and avg pooling layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Create the modified model with loaded weights\n",
    "modified_resnet50 = ResNet50Modified(resnet50)\n",
    "\n",
    "# Put the model in evaluation mode if you are doing inference or feature extraction\n",
    "modified_resnet50.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modified_resnet50 = modified_resnet50.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "ResNet50Modified (ResNet50Modified)           [1, 3, 227, 227]     [1, 2048, 1, 1]      --                   True\n",
       "├─Sequential (features)                       [1, 3, 227, 227]     [1, 2048, 1, 1]      --                   True\n",
       "│    └─Conv2d (0)                             [1, 3, 227, 227]     [1, 64, 114, 114]    9,408                True\n",
       "│    └─BatchNorm2d (1)                        [1, 64, 114, 114]    [1, 64, 114, 114]    128                  True\n",
       "│    └─ReLU (2)                               [1, 64, 114, 114]    [1, 64, 114, 114]    --                   --\n",
       "│    └─MaxPool2d (3)                          [1, 64, 114, 114]    [1, 64, 57, 57]      --                   --\n",
       "│    └─Sequential (4)                         [1, 64, 57, 57]      [1, 256, 57, 57]     --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 64, 57, 57]      [1, 256, 57, 57]     75,008               True\n",
       "│    │    └─Bottleneck (1)                    [1, 256, 57, 57]     [1, 256, 57, 57]     70,400               True\n",
       "│    │    └─Bottleneck (2)                    [1, 256, 57, 57]     [1, 256, 57, 57]     70,400               True\n",
       "│    └─Sequential (5)                         [1, 256, 57, 57]     [1, 512, 29, 29]     --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 256, 57, 57]     [1, 512, 29, 29]     379,392              True\n",
       "│    │    └─Bottleneck (1)                    [1, 512, 29, 29]     [1, 512, 29, 29]     280,064              True\n",
       "│    │    └─Bottleneck (2)                    [1, 512, 29, 29]     [1, 512, 29, 29]     280,064              True\n",
       "│    │    └─Bottleneck (3)                    [1, 512, 29, 29]     [1, 512, 29, 29]     280,064              True\n",
       "│    └─Sequential (6)                         [1, 512, 29, 29]     [1, 1024, 15, 15]    --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 512, 29, 29]     [1, 1024, 15, 15]    1,512,448            True\n",
       "│    │    └─Bottleneck (1)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (2)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (3)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (4)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (5)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    └─Sequential (7)                         [1, 1024, 15, 15]    [1, 2048, 8, 8]      --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 1024, 15, 15]    [1, 2048, 8, 8]      6,039,552            True\n",
       "│    │    └─Bottleneck (1)                    [1, 2048, 8, 8]      [1, 2048, 8, 8]      4,462,592            True\n",
       "│    │    └─Bottleneck (2)                    [1, 2048, 8, 8]      [1, 2048, 8, 8]      4,462,592            True\n",
       "│    └─AdaptiveAvgPool2d (8)                  [1, 2048, 8, 8]      [1, 2048, 1, 1]      --                   --\n",
       "=============================================================================================================================\n",
       "Total params: 23,508,032\n",
       "Trainable params: 23,508,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.63\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.62\n",
       "Forward/backward pass size (MB): 191.90\n",
       "Params size (MB): 94.03\n",
       "Estimated Total Size (MB): 286.55\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=modified_resnet50, input_size=(1, 3, 227, 227), col_width=20,\n",
    "                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset VOCDetection\n",
       "    Number of datapoints: 5717\n",
       "    Root location: ./data"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image load from the VOC dataset and in numpy format\n",
    "\n",
    "image, target = voc_dataset[0]\n",
    "image_array = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with selective search\n",
      "Done with annotations\n",
      "Done with feature extraction\n",
      "Done with label assignment\n",
      "Done with dataset preparation\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def batch_process_features(model, regions):\n",
    "    \"\"\" Process a batch of regions through the model \"\"\"\n",
    "    # Convert list of PIL images to tensor\n",
    "    regions_tensor = torch.stack([preprocess(region) for region in regions]).cuda()\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model(regions_tensor)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "gs = selective_search.get_selective_search()\n",
    "\n",
    "\n",
    "# Assuming 'voc_dataset' is iterable with (image, annot)\n",
    "for image, annot in voc_dataset:\n",
    "    # Get proposals and annotations similar to your original code\n",
    "    image_array = np.array(image)\n",
    "    selective_search.config(gs, image_array, strategy='q')\n",
    "    rects = selective_search.get_rects(gs)\n",
    "    print('Done with selective search')\n",
    "\n",
    "    # Prepare annotations\n",
    "    annotations = []\n",
    "    for obj in annot['annotation']['object']:\n",
    "        annotations.append({\n",
    "            'class': obj['name'],\n",
    "            'bbox': [int(obj['bndbox']['xmin']), int(obj['bndbox']['ymin']), int(obj['bndbox']['xmax']), int(obj['bndbox']['ymax'])]\n",
    "        })\n",
    "    print('Done with annotations')\n",
    "\n",
    "    # Collect regions for batch processing\n",
    "    regions = [image.crop((x1, y1, x2, y2)) for x1, y1, x2, y2 in rects]\n",
    "\n",
    "    # Process all regions at once in batches\n",
    "    features = []\n",
    "    for i in range(0, len(regions), 32):  # batch_size e.g., 32\n",
    "        batch = regions[i:i + 32]\n",
    "        batch_features = batch_process_features(modified_resnet50, batch)\n",
    "        features.extend(batch_features)\n",
    "\n",
    "    print('Done with feature extraction')\n",
    "\n",
    "    # Calculate IOUs and assign labels after extracting all features\n",
    "    labels = []\n",
    "    for proposal, feature in zip(rects, features):\n",
    "        proposal_iou = {cls['class']: calculate_iou(proposal, cls['bbox']) for cls in annotations}\n",
    "        best_iou = max(proposal_iou.values())\n",
    "        if best_iou > 0.5:\n",
    "            matched_class = max(proposal_iou, key=proposal_iou.get)\n",
    "            labels.append((proposal, matched_class, 'positive'))\n",
    "        else:\n",
    "            labels.append((proposal, None, 'negative'))\n",
    "\n",
    "    print('Done with label assignment')\n",
    "\n",
    "    # Prepare datasets for SVM and regression\n",
    "    svm_dataset = []\n",
    "    regression_dataset = []\n",
    "    for feature, label in zip(features, labels):\n",
    "        if label[2] == 'positive':\n",
    "            svm_dataset.append((feature, label[1]))\n",
    "            ground_truth_bbox = next((item['bbox'] for item in annotations if item['class'] == label[1]), None)\n",
    "            if ground_truth_bbox:\n",
    "                tx = ground_truth_bbox[0] - label[0][0]\n",
    "                ty = ground_truth_bbox[1] - label[0][1]\n",
    "                tw = ground_truth_bbox[2] - label[0][2]\n",
    "                th = ground_truth_bbox[3] - label[0][3]\n",
    "                regression_dataset.append((feature, (tx, ty, tw, th)))\n",
    "    \n",
    "    print('Done with dataset preparation')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 1, 1)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with selective search\n",
      "Done with annotations\n",
      "Done with feature extraction and labelling\n",
      "Done with dataset preparation\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "gs = selective_search.get_selective_search()\n",
    "\n",
    "for image, annnot in voc_dataset:\n",
    "\n",
    "    # Get proposals from selective search\n",
    "    image_array = np.array(image)\n",
    "    selective_search.config(gs, image_array, strategy='q')\n",
    "    rects = selective_search.get_rects(gs)\n",
    "    print('Done with selective search')\n",
    "\n",
    "    # Prepare annotations\n",
    "    annotations = []\n",
    "    for obj in annnot['annotation']['object']:\n",
    "        annotations.append({\n",
    "            'class': obj['name'],\n",
    "            'bbox': [int(obj['bndbox']['xmin']), int(obj['bndbox']['ymin']), int(obj['bndbox']['xmax']), int(obj['bndbox']['ymax'])]\n",
    "        })\n",
    "    print('Done with annotations')\n",
    "\n",
    "    # Feature extraction per image \n",
    "\n",
    "    features = []\n",
    "    lables = []\n",
    "    for proposal in rects:\n",
    "        \n",
    "        x1, y1, x2, y2 = proposal\n",
    "        region = image.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        # Make it (1, 3, 227, 227)\n",
    "        region = preprocess(region).unsqueeze(0)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            region_preprocessed = region.cuda()\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            feature = modified_resnet50(region_preprocessed)\n",
    "            features.append(feature.cpu().numpy().flatten())  # Flatten the features\n",
    "        \n",
    "        # Calculate IOU and assign the class\n",
    "        proposal_iou = {cls['class']: calculate_iou(proposal, cls['bbox']) for cls in annotations}\n",
    "        best_iou = max(proposal_iou.values())\n",
    "\n",
    "        if best_iou > 0.5:\n",
    "            matched_class = max(proposal_iou, key=proposal_iou.get)\n",
    "            lables.append((proposal, matched_class, 'positive'))\n",
    "        else:\n",
    "            lables.append((proposal, None, 'negative'))\n",
    "\n",
    "    print('Done with feature extraction and labelling')\n",
    "\n",
    "    svm_dataset = []\n",
    "    regression_dataset = []\n",
    "\n",
    "    for feature, label in zip(features, lables):\n",
    "        if label[2] == 'positive':\n",
    "            svm_dataset.append((feature, label[1]))\n",
    "\n",
    "            ground_truth_bbox = next((item['bbox'] for item in annotations if item['class'] == matched_class), None)\n",
    "            if ground_truth_bbox:\n",
    "                # Calculate offsets\n",
    "                gx1, gy1, gx2, gy2 = ground_truth_bbox\n",
    "                px1, py1, px2, py2 = proposal\n",
    "                tx = gx1 - px1\n",
    "                ty = gy1 - py1\n",
    "                tw = gx2 - px2\n",
    "                th = gy2 - py2\n",
    "                regression_dataset.append((feature, (tx, ty, tw, th)))\n",
    "    \n",
    "    print('Done with dataset preparation')\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
