{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet, resnet50\n",
    "from torchvision.transforms import functional as F\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# models from torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import RoIPool\n",
    "\n",
    "sys.path.append('../../')  \n",
    "from Object_Detection.RCNN import selective_search\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Get Fast RCNN from torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(annotation):\n",
    "    objects = annotation['annotation']['object']\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in objects:\n",
    "        xmin = float(obj['bndbox']['xmin'])\n",
    "        ymin = float(obj['bndbox']['ymin'])\n",
    "        xmax = float(obj['bndbox']['xmax'])\n",
    "        ymax = float(obj['bndbox']['ymax'])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        classes.append(obj['name'])\n",
    "    return torch.tensor(boxes), classes  # Convert boxes to tensors, keep classes as list or map them to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar to ./data/VOCdevkit/VOC2012\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  # Resize images to a common size\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomVOCDataset(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        boxes, classes = parse_annotations(target)\n",
    "        return img, boxes, classes\n",
    "\n",
    "# Update the dataset instance with the custom class\n",
    "dataset = CustomVOCDataset(root='./data/VOCdevkit/VOC2012', year='2012', image_set='train', download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proposals: 2914\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([2914, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([2914, 100352])\n",
      "Number of proposals: 4192\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([4192, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([4192, 100352])\n",
      "Run the loop once\n",
      "Epoch [1/10], Loss: 0.35300503074028117\n",
      "Number of proposals: 5314\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([5314, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([5314, 100352])\n",
      "Number of proposals: 746\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([746, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([746, 100352])\n",
      "Run the loop once\n",
      "Epoch [2/10], Loss: 0.9378403534688211\n",
      "Number of proposals: 3681\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([3681, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([3681, 100352])\n",
      "Number of proposals: 4587\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([4587, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([4587, 100352])\n",
      "Run the loop once\n",
      "Epoch [3/10], Loss: 0.28809654418667735\n",
      "Number of proposals: 717\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([717, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([717, 100352])\n",
      "Number of proposals: 5424\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([5424, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([5424, 100352])\n",
      "Run the loop once\n",
      "Epoch [4/10], Loss: 0.4162674177062038\n",
      "Number of proposals: 2008\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([2008, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([2008, 100352])\n",
      "Number of proposals: 2681\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([2681, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([2681, 100352])\n",
      "Run the loop once\n",
      "Epoch [5/10], Loss: 0.384082740141792\n",
      "Number of proposals: 5847\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([5847, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([5847, 100352])\n",
      "Number of proposals: 6123\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([6123, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([6123, 100352])\n",
      "Run the loop once\n",
      "Epoch [6/10], Loss: 0.15494593803294998\n",
      "Number of proposals: 2382\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([2382, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([2382, 100352])\n",
      "Number of proposals: 6170\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([6170, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([6170, 100352])\n",
      "Run the loop once\n",
      "Epoch [7/10], Loss: 0.45801753840997794\n",
      "Number of proposals: 3157\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([3157, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([3157, 100352])\n",
      "Number of proposals: 3080\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([3080, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([3080, 100352])\n",
      "Run the loop once\n",
      "Epoch [8/10], Loss: 0.5509947716241036\n",
      "Number of proposals: 6109\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([6109, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([6109, 100352])\n",
      "Number of proposals: 3233\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([3233, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([3233, 100352])\n",
      "Run the loop once\n",
      "Epoch [9/10], Loss: 0.18494574757166127\n",
      "Number of proposals: 1031\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([1031, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([1031, 100352])\n",
      "Number of proposals: 5898\n",
      "Feature map shape: torch.Size([1, 2048, 25, 25])\n",
      "RoI pooled feature shape: torch.Size([5898, 2048, 7, 7])\n",
      "Flattened RoI pooled feature shape: torch.Size([5898, 100352])\n",
      "Run the loop once\n",
      "Epoch [10/10], Loss: 0.38669772879610703\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.ops import RoIPool\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Define the backbone network\n",
    "weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "base_model = resnet50(weights=weights)\n",
    "base_model = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "# Define the RoI Pooling and classification head\n",
    "roi_pool = RoIPool(output_size=(7, 7), spatial_scale=1.0 / 16)\n",
    "\n",
    "num_classes = 21  # PASCAL VOC classes + background\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(2048 * 7 * 7, 4096),  # Adjusted based on the RoI Pooling output\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, num_classes + 4)  # class scores and bbox regressor outputs\n",
    ")\n",
    "\n",
    "gs = selective_search.get_selective_search()\n",
    "\n",
    "# Label encoder to convert string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n",
    "                   'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'background'])\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_reg = nn.SmoothL1Loss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "classifier.to(device)\n",
    "base_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    base_model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (images, annotations, targets) in enumerate(data_loader):\n",
    "\n",
    "        # Generate region proposals using selective search\n",
    "        image_array = images.permute(0, 2, 3, 1).numpy()\n",
    "        image_array = np.array(image_array)[0]\n",
    "        selective_search.config(gs, image_array, strategy='q')\n",
    "        rects = selective_search.get_rects(gs)        \n",
    "        print(\"Number of proposals:\", len(rects))\n",
    "\n",
    "        # Convert rects to the format expected by RoI Pooling\n",
    "        rects = [[r[0], r[1], r[0] + r[2], r[1] + r[3]] for r in rects]\n",
    "        rects = torch.tensor(rects).float().to(device)\n",
    "\n",
    "\n",
    "        images = images.to(device)\n",
    "        annotations = [annot.to(device) for annot in annotations]  # Assuming boxes is a list of tensors\n",
    "        \n",
    "        # Encode the string labels to integers\n",
    "        label_targets = [torch.tensor(label_encoder.transform(target), dtype=torch.long).to(device) for target in targets]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = base_model(images)\n",
    "        print(\"Feature map shape:\", features.shape)  # Debugging feature map shape\n",
    "        \n",
    "        # Apply RoI pooling\n",
    "        roi_pooled_features = roi_pool(features, [rects])\n",
    "        print(\"RoI pooled feature shape:\", roi_pooled_features.shape)  # Debugging RoI pooled feature shape\n",
    "        \n",
    "        # Flatten RoI pooled features\n",
    "        roi_pooled_features = roi_pooled_features.view(roi_pooled_features.size(0), -1)\n",
    "        print(\"Flattened RoI pooled feature shape:\", roi_pooled_features.shape)  # Debugging flattened feature shape\n",
    "\n",
    "        # Forward pass through the classifier\n",
    "        outputs = classifier(roi_pooled_features)\n",
    "\n",
    "        # Split outputs into class scores and bounding box regressions\n",
    "        cls_scores = outputs[:, :num_classes]\n",
    "        bbox_regressions = outputs[:, num_classes:]\n",
    "\n",
    "        # Compute losses for each RoI\n",
    "        cls_loss = 0\n",
    "        reg_loss = 0\n",
    "\n",
    "        for j in range(len(label_targets)):\n",
    "            cls_loss += criterion_cls(cls_scores[j].unsqueeze(0), label_targets[j])\n",
    "            \n",
    "            target_bbox = rects[j].view(1, -1)  # Ensure target is (1, 4)\n",
    "            pred_bbox = bbox_regressions[j].view(1, -1)  # Ensure prediction is (1, 4)\n",
    "            reg_loss += criterion_reg(pred_bbox, target_bbox)\n",
    "\n",
    "        loss = cls_loss + reg_loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i == 1:  # Just for debugging\n",
    "            print(\"Run the loop once\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
