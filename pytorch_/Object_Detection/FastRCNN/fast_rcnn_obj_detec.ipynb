{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet, resnet50\n",
    "from torchvision.transforms import functional as F\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# models from torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import RoIPool\n",
    "\n",
    "sys.path.append('../../')  \n",
    "from Object_Detection.RCNN import selective_search\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Get Fast RCNN from torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(annotation):\n",
    "    objects = annotation['annotation']['object']\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in objects:\n",
    "        xmin = float(obj['bndbox']['xmin'])\n",
    "        ymin = float(obj['bndbox']['ymin'])\n",
    "        xmax = float(obj['bndbox']['xmax'])\n",
    "        ymax = float(obj['bndbox']['ymax'])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        classes.append(obj['name'])\n",
    "    return torch.tensor(boxes), classes  # Convert boxes to tensors, keep classes as list or map them to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar to ./data/VOCdevkit/VOC2012\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  # Resize images to a common size\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomVOCDataset(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        boxes, classes = parse_annotations(target)\n",
    "        return img, boxes, classes\n",
    "\n",
    "# Update the dataset instance with the custom class\n",
    "dataset = CustomVOCDataset(root='./data/VOCdevkit/VOC2012', year='2012', image_set='train', download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, boxes, classes = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = models.resnet50(weights=True)\n",
    "\n",
    "# Remove the final fully connected layer (fc) to use as a feature extractor\n",
    "modules = list(base_model.children())[:-2]  # Remove the last fully connected layer and avgpool\n",
    "base_model = torch.nn.Sequential(*modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate RoI pooling layer that takes selected region proposals and extracts features\n",
    "# RoI pooling\n",
    "roi_pool = RoIPool(output_size=(7, 7), spatial_scale=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 21  # PASCAL VOC classes + background\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(512 * 7 * 7, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, num_classes + 4)  # class scores and bbox regressor outputs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
