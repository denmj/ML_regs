{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet, resnet50\n",
    "from torchvision.transforms import functional as F\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# models from torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import RoIPool\n",
    "\n",
    "sys.path.append('../../')  \n",
    "from Object_Detection.RCNN import selective_search\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Get Fast RCNN from torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(annotation):\n",
    "    objects = annotation['annotation']['object']\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in objects:\n",
    "        xmin = float(obj['bndbox']['xmin'])\n",
    "        ymin = float(obj['bndbox']['ymin'])\n",
    "        xmax = float(obj['bndbox']['xmax'])\n",
    "        ymax = float(obj['bndbox']['ymax'])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        classes.append(obj['name'])\n",
    "    return torch.tensor(boxes), classes  # Convert boxes to tensors, keep classes as list or map them to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar to ./data/VOCdevkit/VOC2012\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  # Resize images to a common size\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomVOCDataset(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        boxes, classes = parse_annotations(target)\n",
    "        return img, boxes, classes\n",
    "\n",
    "# Update the dataset instance with the custom class\n",
    "dataset = CustomVOCDataset(root='./data/VOCdevkit/VOC2012', year='2012', image_set='train', download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, boxes, classes = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2489,  2.2489,  2.2489,  ...,  1.3584,  1.3755,  1.3755],\n",
       "         [ 2.2489,  2.2489,  2.2489,  ...,  1.3584,  1.3584,  1.3584],\n",
       "         [ 2.2489,  2.2489,  2.2489,  ...,  1.3584,  1.3413,  1.3413],\n",
       "         ...,\n",
       "         [ 1.1358,  1.1358,  1.1529,  ..., -0.3369, -0.2171, -0.1143],\n",
       "         [ 1.0159,  1.0159,  1.0331,  ..., -0.6623, -0.5767, -0.5082],\n",
       "         [ 0.9303,  0.9474,  0.9646,  ..., -0.8507, -0.7993, -0.7479]],\n",
       "\n",
       "        [[ 2.4286,  2.4286,  2.4286,  ...,  1.5707,  1.5882,  1.5882],\n",
       "         [ 2.4286,  2.4286,  2.4286,  ...,  1.5707,  1.5707,  1.5707],\n",
       "         [ 2.4286,  2.4286,  2.4286,  ...,  1.5707,  1.5532,  1.5532],\n",
       "         ...,\n",
       "         [ 1.0455,  1.0455,  1.0630,  ..., -0.3025, -0.1625, -0.0399],\n",
       "         [ 0.9230,  0.9230,  0.9405,  ..., -0.6352, -0.5301, -0.4426],\n",
       "         [ 0.8354,  0.8529,  0.8704,  ..., -0.8277, -0.7577, -0.6877]],\n",
       "\n",
       "        [[ 2.6400,  2.6400,  2.6400,  ...,  2.5354,  2.5529,  2.5529],\n",
       "         [ 2.6400,  2.6400,  2.6400,  ...,  2.5354,  2.5354,  2.5354],\n",
       "         [ 2.6400,  2.6400,  2.6400,  ...,  2.5354,  2.5180,  2.5180],\n",
       "         ...,\n",
       "         [ 1.0365,  1.0365,  1.0539,  ..., -0.1835, -0.0615,  0.0256],\n",
       "         [ 0.9145,  0.9145,  0.9319,  ..., -0.4798, -0.4275, -0.3578],\n",
       "         [ 0.8274,  0.8448,  0.8622,  ..., -0.6715, -0.6367, -0.5844]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opencv switchToSelectiveSearchQuality takes not [3, H, W] but [H, W, 3]\n",
    "img_for_sc = img.permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check selectivesearch\n",
    "\n",
    "gs = selective_search.get_selective_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_array = np.array(img)\n",
    "selective_search.config(gs, img_for_sc, strategy='q')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "base_model = models.resnet50(weights=True)\n",
    "\n",
    "# Remove the final fully connected layer (fc) to use as a feature extractor\n",
    "modules = list(base_model.children())[:-2]  # Remove the last fully connected layer and avgpool\n",
    "base_model = torch.nn.Sequential(*modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate RoI pooling layer that takes selected region proposals and extracts features\n",
    "# RoI pooling\n",
    "roi_pool = RoIPool(output_size=(7, 7), spatial_scale=1/32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rects = selective_search.get_rects(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176, 566, 223, 585])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = base_model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 25, 25])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the featire through the RoI pooling layer with the region proposals\n",
    "roi_pool_feats = roi_pool(f, [torch.tensor([rects[0]]).float()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9616, 0.9616, 0.9616,  ..., 1.2233, 1.2233, 1.2233],\n",
       "          [0.9616, 0.9616, 0.9616,  ..., 1.2233, 1.2233, 1.2233],\n",
       "          [0.9616, 0.9616, 0.9616,  ..., 1.2233, 1.2233, 1.2233],\n",
       "          ...,\n",
       "          [0.9616, 0.9616, 0.9616,  ..., 1.2233, 1.2233, 1.2233],\n",
       "          [0.9616, 0.9616, 0.9616,  ..., 1.2233, 1.2233, 1.2233],\n",
       "          [0.9616, 0.9616, 0.9616,  ..., 1.2233, 1.2233, 1.2233]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.3243, 0.3243, 0.3243],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3243, 0.3243, 0.3243],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3243, 0.3243, 0.3243],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3243, 0.3243, 0.3243],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3243, 0.3243, 0.3243],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3243, 0.3243, 0.3243]],\n",
       "\n",
       "         [[0.8508, 0.8508, 0.8508,  ..., 1.3069, 1.3069, 1.3069],\n",
       "          [0.8508, 0.8508, 0.8508,  ..., 1.3069, 1.3069, 1.3069],\n",
       "          [0.8508, 0.8508, 0.8508,  ..., 1.3069, 1.3069, 1.3069],\n",
       "          ...,\n",
       "          [0.8508, 0.8508, 0.8508,  ..., 1.3069, 1.3069, 1.3069],\n",
       "          [0.8508, 0.8508, 0.8508,  ..., 1.3069, 1.3069, 1.3069],\n",
       "          [0.8508, 0.8508, 0.8508,  ..., 1.3069, 1.3069, 1.3069]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0209, 1.0209, 1.0209,  ..., 1.2468, 1.2468, 1.2468],\n",
       "          [1.0209, 1.0209, 1.0209,  ..., 1.2468, 1.2468, 1.2468],\n",
       "          [1.0209, 1.0209, 1.0209,  ..., 1.2468, 1.2468, 1.2468],\n",
       "          ...,\n",
       "          [1.0209, 1.0209, 1.0209,  ..., 1.2468, 1.2468, 1.2468],\n",
       "          [1.0209, 1.0209, 1.0209,  ..., 1.2468, 1.2468, 1.2468],\n",
       "          [1.0209, 1.0209, 1.0209,  ..., 1.2468, 1.2468, 1.2468]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0443, 0.0443, 0.0443,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0443, 0.0443, 0.0443,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0443, 0.0443, 0.0443,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0443, 0.0443, 0.0443,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0443, 0.0443, 0.0443,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0443, 0.0443, 0.0443,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
       "       grad_fn=<ROIPoolFunction>>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_pool_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 21  # PASCAL VOC classes + background\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(512 * 7 * 7, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, num_classes + 4)  # class scores and bbox regressor outputs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
