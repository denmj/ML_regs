{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet, resnet50\n",
    "from torchvision.transforms import functional as F\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# models from torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import RoIPool\n",
    "\n",
    "sys.path.append('../../')  \n",
    "from Object_Detection.RCNN import selective_search\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Get Fast RCNN from torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(annotation):\n",
    "    objects = annotation['annotation']['object']\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in objects:\n",
    "        xmin = float(obj['bndbox']['xmin'])\n",
    "        ymin = float(obj['bndbox']['ymin'])\n",
    "        xmax = float(obj['bndbox']['xmax'])\n",
    "        ymax = float(obj['bndbox']['ymax'])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        classes.append(obj['name'])\n",
    "    return torch.tensor(boxes), classes  # Convert boxes to tensors, keep classes as list or map them to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar to ./data/VOCdevkit/VOC2012\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  # Resize images to a common size\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomVOCDataset(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        boxes, classes = parse_annotations(target)\n",
    "        return img, boxes, classes\n",
    "\n",
    "# Update the dataset instance with the custom class\n",
    "dataset = CustomVOCDataset(root='./data/VOCdevkit/VOC2012', year='2012', image_set='train', download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1\n",
      "torch.Size([1, 3, 800, 800])\n",
      "(800, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "gs = selective_search.get_selective_search()\n",
    "\n",
    "counter, start_index = 0, 0\n",
    "\n",
    "# Assuming 'voc_dataset' is iterable with (image, annot)\n",
    "for image, boxes, classes in data_loader:\n",
    "    \n",
    "    counter += 1\n",
    "    # Counter for images processed\n",
    "    print(f'Processing image {counter}')\n",
    "\n",
    "    print(image.shape)\n",
    "    # Get proposals and annotations similar to your original code\n",
    "    image_array = image.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "    image_array = np.array(image_array)[0]\n",
    "    print(image_array.shape)\n",
    "    selective_search.config(gs, image_array, strategy='q')\n",
    "    rects = selective_search.get_rects(gs)\n",
    "\n",
    "\n",
    "    if counter == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "base_model = resnet50(pretrained=True)\n",
    "# Remove the final classification layers\n",
    "base_model = nn.Sequential(*list(base_model.children())[:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate RoI pooling layer that takes selected region proposals and extracts features\n",
    "# RoI pooling\n",
    "roi_pool = RoIPool(output_size=(7, 7), spatial_scale=1.0/16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 21  # PASCAL VOC classes + background\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(512 * 7 * 7, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, num_classes + 4)  # class scores and bbox regressor outputs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "classifier.to(device)\n",
    "base_model.to(device)\n",
    "\n",
    "# Label encoder to convert string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n",
    "                   'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'background'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the backbone network\n",
    "base_model = resnet50(pretrained=True)\n",
    "base_model = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "# Define the RoI Pooling and classification head\n",
    "roi_pool = RoIPool(output_size=(7, 7), spatial_scale=1.0 / 16)\n",
    "\n",
    "num_classes = 21  # PASCAL VOC classes + background\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(2048 * 7 * 7, 4096),  # Adjusted based on the RoI Pooling output\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, num_classes + 4)  # class scores and bbox regressor outputs\n",
    ")\n",
    "\n",
    "\n",
    "# Label encoder to convert string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n",
    "                   'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'background'])\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_reg = nn.SmoothL1Loss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "classifier.to(device)\n",
    "base_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    base_model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (images, boxes, targets) in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "        boxes = [box.to(device) for box in boxes]  # Assuming boxes is a list of tensors\n",
    "        \n",
    "        # Encode the string labels to integers\n",
    "        label_targets = [torch.tensor(label_encoder.transform(target)).to(device) for target in targets]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = base_model(images)\n",
    "        print(\"Feature map shape:\", features.shape)  # Debugging feature map shape\n",
    "        \n",
    "        # Apply RoI pooling\n",
    "        roi_pooled_features = roi_pool(features, boxes)\n",
    "        print(\"RoI pooled feature shape:\", roi_pooled_features.shape)  # Debugging RoI pooled feature shape\n",
    "        \n",
    "        # Flatten RoI pooled features\n",
    "        roi_pooled_features = roi_pooled_features.view(roi_pooled_features.size(0), -1)\n",
    "        print(\"Flattened RoI pooled feature shape:\", roi_pooled_features.shape)  # Debugging flattened feature shape\n",
    "\n",
    "        # Forward pass through the classifier\n",
    "        outputs = classifier(roi_pooled_features)\n",
    "\n",
    "        # Split outputs into class scores and bounding box regressions\n",
    "        cls_scores = outputs[:, :num_classes]\n",
    "        bbox_regressions = outputs[:, num_classes:]\n",
    "\n",
    "        # Compute losses for each RoI\n",
    "        cls_loss = 0\n",
    "        reg_loss = 0\n",
    "        if i == 1:\n",
    "            print(\"Run the loop once\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
