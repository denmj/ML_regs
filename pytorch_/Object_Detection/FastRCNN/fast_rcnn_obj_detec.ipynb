{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet, resnet50\n",
    "from torchvision.transforms import functional as F\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# models from torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import RoIPool\n",
    "\n",
    "sys.path.append('../../')  \n",
    "from Object_Detection.RCNN import selective_search\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Get Fast RCNN from torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(annotation):\n",
    "    objects = annotation['annotation']['object']\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in objects:\n",
    "        xmin = float(obj['bndbox']['xmin'])\n",
    "        ymin = float(obj['bndbox']['ymin'])\n",
    "        xmax = float(obj['bndbox']['xmax'])\n",
    "        ymax = float(obj['bndbox']['ymax'])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        classes.append(obj['name'])\n",
    "    return torch.tensor(boxes), classes  # Convert boxes to tensors, keep classes as list or map them to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCdevkit/VOC2012\\VOCtrainval_11-May-2012.tar to ./data/VOCdevkit/VOC2012\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  # Resize images to a common size\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomVOCDataset(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        boxes, classes = parse_annotations(target)\n",
    "        return img, boxes, classes\n",
    "\n",
    "# Update the dataset instance with the custom class\n",
    "dataset = CustomVOCDataset(root='./data/VOCdevkit/VOC2012', year='2012', image_set='train', download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1\n",
      "torch.Size([1, 3, 800, 800])\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0xcf03815f::Set<3,-1,-1>,struct cv::impl::A0xcf03815f::Set<0,5,-1>,4>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m image_array \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     16\u001b[0m image_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image_array)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mselective_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m rects \u001b[38;5;241m=\u001b[39m selective_search\u001b[38;5;241m.\u001b[39mget_rects(gs)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\pytorch_\\Object_Detection\\FastRCNN\\../..\\Object_Detection\\RCNN\\selective_search.py:19\u001b[0m, in \u001b[0;36mconfig\u001b[1;34m(gs, img, strategy)\u001b[0m\n\u001b[0;32m     17\u001b[0m     gs\u001b[38;5;241m.\u001b[39mswitchToSelectiveSearchFast()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswitchToSelectiveSearchQuality\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;18m__doc__\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0xcf03815f::Set<3,-1,-1>,struct cv::impl::A0xcf03815f::Set<0,5,-1>,4>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "gs = selective_search.get_selective_search()\n",
    "\n",
    "counter, start_index = 0, 0\n",
    "\n",
    "# Assuming 'voc_dataset' is iterable with (image, annot)\n",
    "for image, boxes, classes in data_loader:\n",
    "    \n",
    "    counter += 1\n",
    "    # Counter for images processed\n",
    "    print(f'Processing image {counter}')\n",
    "\n",
    "    print(image.shape)\n",
    "    # Get proposals and annotations similar to your original code\n",
    "    image_array = image.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "    image_array = np.array(image_array)\n",
    "    selective_search.config(gs, image_array, strategy='q')\n",
    "    rects = selective_search.get_rects(gs)\n",
    "\n",
    "\n",
    "    if counter == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check selectivesearch\n",
    "\n",
    "gs = selective_search.get_selective_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_array = np.array(img)\n",
    "selective_search.config(gs, img_for_sc, strategy='q')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "base_model = models.resnet50(weights=True)\n",
    "\n",
    "# Remove the final fully connected layer (fc) to use as a feature extractor\n",
    "modules = list(base_model.children())[:-2]  # Remove the last fully connected layer and avgpool\n",
    "base_model = torch.nn.Sequential(*modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate RoI pooling layer that takes selected region proposals and extracts features\n",
    "# RoI pooling\n",
    "roi_pool = RoIPool(output_size=(7, 7), spatial_scale=1/32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rects = selective_search.get_rects(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176, 566, 223, 585])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = base_model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 25, 25])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the featire through the RoI pooling layer with the region proposals\n",
    "roi_pool_feats = roi_pool(f, [torch.tensor([rects[0]]).float()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pool_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 21  # PASCAL VOC classes + background\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(512 * 7 * 7, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, 4096),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(4096, num_classes + 4)  # class scores and bbox regressor outputs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
