{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import VOCDetection\n",
    "\n",
    "sys.path.append('../')  \n",
    "from Object_Detection import selective_search\n",
    "from Object_Detection.bbox import calculate_iou\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Settings \n",
    "\n",
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data\\VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "voc_dataset = VOCDetection(root='./data', \n",
    "                           year='2012', \n",
    "                           image_set='train',\n",
    "                           download=True, \n",
    "                        #    transform=transform\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and its finetuned weights (PASCAL VOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the ResNet50 model without pretrained weights\n",
    "resnet50 = models.resnet50(weights=False)\n",
    "\n",
    "resnet50.fc = torch.nn.Linear(resnet50.fc.in_features, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load your finetuned weights\n",
    "# Make sure that the path 'voc_finetune_resnet_weights.pth' is correct and accessible\n",
    "resnet50.load_state_dict(torch.load('resnet50_pascal_voc.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model=resnet50, input_size=(1, 3, 227, 227), col_width=20,\n",
    "                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Modified(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ResNet50Modified, self).__init__()\n",
    "        # Adapt this line if your model was changed differently\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-1])  # Typically removes the final fully connected layer and avg pooling layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Create the modified model with loaded weights\n",
    "modified_resnet50 = ResNet50Modified(resnet50)\n",
    "\n",
    "# Put the model in evaluation mode if you are doing inference or feature extraction\n",
    "modified_resnet50.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modified_resnet50 = modified_resnet50.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "ResNet50Modified (ResNet50Modified)           [1, 3, 227, 227]     [1, 2048, 1, 1]      --                   True\n",
       "├─Sequential (features)                       [1, 3, 227, 227]     [1, 2048, 1, 1]      --                   True\n",
       "│    └─Conv2d (0)                             [1, 3, 227, 227]     [1, 64, 114, 114]    9,408                True\n",
       "│    └─BatchNorm2d (1)                        [1, 64, 114, 114]    [1, 64, 114, 114]    128                  True\n",
       "│    └─ReLU (2)                               [1, 64, 114, 114]    [1, 64, 114, 114]    --                   --\n",
       "│    └─MaxPool2d (3)                          [1, 64, 114, 114]    [1, 64, 57, 57]      --                   --\n",
       "│    └─Sequential (4)                         [1, 64, 57, 57]      [1, 256, 57, 57]     --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 64, 57, 57]      [1, 256, 57, 57]     75,008               True\n",
       "│    │    └─Bottleneck (1)                    [1, 256, 57, 57]     [1, 256, 57, 57]     70,400               True\n",
       "│    │    └─Bottleneck (2)                    [1, 256, 57, 57]     [1, 256, 57, 57]     70,400               True\n",
       "│    └─Sequential (5)                         [1, 256, 57, 57]     [1, 512, 29, 29]     --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 256, 57, 57]     [1, 512, 29, 29]     379,392              True\n",
       "│    │    └─Bottleneck (1)                    [1, 512, 29, 29]     [1, 512, 29, 29]     280,064              True\n",
       "│    │    └─Bottleneck (2)                    [1, 512, 29, 29]     [1, 512, 29, 29]     280,064              True\n",
       "│    │    └─Bottleneck (3)                    [1, 512, 29, 29]     [1, 512, 29, 29]     280,064              True\n",
       "│    └─Sequential (6)                         [1, 512, 29, 29]     [1, 1024, 15, 15]    --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 512, 29, 29]     [1, 1024, 15, 15]    1,512,448            True\n",
       "│    │    └─Bottleneck (1)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (2)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (3)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (4)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    │    └─Bottleneck (5)                    [1, 1024, 15, 15]    [1, 1024, 15, 15]    1,117,184            True\n",
       "│    └─Sequential (7)                         [1, 1024, 15, 15]    [1, 2048, 8, 8]      --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 1024, 15, 15]    [1, 2048, 8, 8]      6,039,552            True\n",
       "│    │    └─Bottleneck (1)                    [1, 2048, 8, 8]      [1, 2048, 8, 8]      4,462,592            True\n",
       "│    │    └─Bottleneck (2)                    [1, 2048, 8, 8]      [1, 2048, 8, 8]      4,462,592            True\n",
       "│    └─AdaptiveAvgPool2d (8)                  [1, 2048, 8, 8]      [1, 2048, 1, 1]      --                   --\n",
       "=============================================================================================================================\n",
       "Total params: 23,508,032\n",
       "Trainable params: 23,508,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.63\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.62\n",
       "Forward/backward pass size (MB): 191.90\n",
       "Params size (MB): 94.03\n",
       "Estimated Total Size (MB): 286.55\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=modified_resnet50, input_size=(1, 3, 227, 227), col_width=20,\n",
    "                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image load from the VOC dataset and in numpy format\n",
    "\n",
    "image, target = voc_dataset[0]\n",
    "image_array = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = selective_search.get_selective_search()\n",
    "\n",
    "selective_search.config(gs, image_array, strategy='q')\n",
    "\n",
    "# returns coordinates as [x1, y1, x2, y2]\n",
    "rects = selective_search.get_rects(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse annotation to this  [{'class': 'dog', 'bbox': [50, 50, 200, 200]}, {'class': 'cat', 'bbox': [220, 220, 350, 350]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for obj in target['annotation']['object']:\n",
    "    annotations.append({\n",
    "        'class': obj['name'],\n",
    "        'bbox': [int(obj['bndbox']['xmin']), int(obj['bndbox']['ymin']), int(obj['bndbox']['xmax']), int(obj['bndbox']['ymax'])]\n",
    "    })\n",
    "    # print(obj['name'], obj['bndbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class': 'horse', 'bbox': [53, 87, 471, 420]},\n",
       " {'class': 'person', 'bbox': [158, 44, 289, 167]}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation': {'folder': 'VOC2012',\n",
       "  'filename': '2008_000008.jpg',\n",
       "  'source': {'database': 'The VOC2008 Database',\n",
       "   'annotation': 'PASCAL VOC2008',\n",
       "   'image': 'flickr'},\n",
       "  'size': {'width': '500', 'height': '442', 'depth': '3'},\n",
       "  'segmented': '0',\n",
       "  'object': [{'name': 'horse',\n",
       "    'pose': 'Left',\n",
       "    'truncated': '0',\n",
       "    'occluded': '1',\n",
       "    'bndbox': {'xmin': '53', 'ymin': '87', 'xmax': '471', 'ymax': '420'},\n",
       "    'difficult': '0'},\n",
       "   {'name': 'person',\n",
       "    'pose': 'Unspecified',\n",
       "    'truncated': '1',\n",
       "    'occluded': '0',\n",
       "    'bndbox': {'xmin': '158', 'ymin': '44', 'xmax': '289', 'ymax': '167'},\n",
       "    'difficult': '0'}]}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['annotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([412, 236, 442, 286])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = calculate_iou(rects[0], [53, 87, 471, 420])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01077632656580025"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each region proposal\n",
    "features = []\n",
    "\n",
    "for x, y, w, h in rects:\n",
    "    # Crop and preprocess the region\n",
    "    region = image.crop((x, y, x + w, y + h))\n",
    "    region_preprocessed = preprocess(region).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        region_preprocessed = region_preprocessed.cuda()\n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        feature = modified_resnet50(region_preprocessed)\n",
    "        features.append(feature.cpu().numpy().flatten())  # Flatten the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = []\n",
    "for obj in target['annotation']['object']:\n",
    "    annotations.append({\n",
    "        'class': obj['name'],\n",
    "        'bbox': [int(obj['bndbox']['xmin']), int(obj['bndbox']['ymin']), int(obj['bndbox']['xmax']), int(obj['bndbox']['ymax'])]\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
