{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Settings \n",
    "\n",
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful constants\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_normalize = transforms.Compose([\n",
    "    # transforms.Resize(32),  # Resize the images to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.1307), (0.30811))  # Normalize with MNIST's mean and std for each channel\n",
    "])\n",
    "\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                download=True, transform=transform_normalize)\n",
    "\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                 download=True, transform=transform_normalize)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_trainset, [50000, 10000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedVGG16, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Adjust the first conv layer to accept 1-channel input\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_MNIST, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10)\n",
    "\n",
    "accuracy = accuracy.to(device)\n",
    "model_vgg16 = VGG16_MNIST().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_adam = optim.Adam(model_vgg16.parameters(), lr=1e-4)\n",
    "\n",
    "optimizer_sgd = torch.optim.SGD(model_vgg16.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================\n",
       "Layer (type (var_name))                  Input Shape       Output Shape      Param #           Trainable\n",
       "============================================================================================================\n",
       "VGG16_MNIST (VGG16_MNIST)                [1, 1, 28, 28]    [1, 10]           --                True\n",
       "├─Sequential (features)                  [1, 1, 28, 28]    [1, 128, 7, 7]    --                True\n",
       "│    └─Conv2d (0)                        [1, 1, 28, 28]    [1, 64, 28, 28]   640               True\n",
       "│    └─ReLU (1)                          [1, 64, 28, 28]   [1, 64, 28, 28]   --                --\n",
       "│    └─Conv2d (2)                        [1, 64, 28, 28]   [1, 64, 28, 28]   36,928            True\n",
       "│    └─ReLU (3)                          [1, 64, 28, 28]   [1, 64, 28, 28]   --                --\n",
       "│    └─MaxPool2d (4)                     [1, 64, 28, 28]   [1, 64, 14, 14]   --                --\n",
       "│    └─Conv2d (5)                        [1, 64, 14, 14]   [1, 128, 14, 14]  73,856            True\n",
       "│    └─ReLU (6)                          [1, 128, 14, 14]  [1, 128, 14, 14]  --                --\n",
       "│    └─Conv2d (7)                        [1, 128, 14, 14]  [1, 128, 14, 14]  147,584           True\n",
       "│    └─ReLU (8)                          [1, 128, 14, 14]  [1, 128, 14, 14]  --                --\n",
       "│    └─MaxPool2d (9)                     [1, 128, 14, 14]  [1, 128, 7, 7]    --                --\n",
       "├─Sequential (classifier)                [1, 6272]         [1, 10]           --                True\n",
       "│    └─Linear (0)                        [1, 6272]         [1, 4096]         25,694,208        True\n",
       "│    └─ReLU (1)                          [1, 4096]         [1, 4096]         --                --\n",
       "│    └─Dropout (2)                       [1, 4096]         [1, 4096]         --                --\n",
       "│    └─Linear (3)                        [1, 4096]         [1, 4096]         16,781,312        True\n",
       "│    └─ReLU (4)                          [1, 4096]         [1, 4096]         --                --\n",
       "│    └─Dropout (5)                       [1, 4096]         [1, 4096]         --                --\n",
       "│    └─Linear (6)                        [1, 4096]         [1, 10]           40,970            True\n",
       "============================================================================================================\n",
       "Total params: 42,775,498\n",
       "Trainable params: 42,775,498\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 115.37\n",
       "============================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.27\n",
       "Params size (MB): 171.10\n",
       "Estimated Total Size (MB): 172.38\n",
       "============================================================================================================"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model_vgg16, input_size=(1, 1, 28, 28), col_width=17,\n",
    "                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the loss and accuracy\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "experiment_name = f'VGG16_MNIST_{timestamp}'\n",
    "model_name = 'Vgg16'\n",
    "log_dir = os.path.join('runs', timestamp, experiment_name, model_name)\n",
    "log_writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64 / 50000\n",
      "Loss: 2.299743413925171, Accuracy: 0.15625\n",
      "Batch 128 / 50000\n",
      "Loss: 4.5951759815216064, Accuracy: 0.1875\n",
      "Batch 192 / 50000\n",
      "Loss: 6.892356634140015, Accuracy: 0.1875\n",
      "Batch 256 / 50000\n",
      "Loss: 9.18633246421814, Accuracy: 0.125\n",
      "Batch 320 / 50000\n",
      "Loss: 11.477254629135132, Accuracy: 0.09375\n",
      "Batch 384 / 50000\n",
      "Loss: 13.778669357299805, Accuracy: 0.078125\n",
      "Batch 448 / 50000\n",
      "Loss: 16.05557656288147, Accuracy: 0.125\n",
      "Batch 512 / 50000\n",
      "Loss: 18.32184648513794, Accuracy: 0.15625\n",
      "Batch 576 / 50000\n",
      "Loss: 20.594024658203125, Accuracy: 0.125\n",
      "Batch 640 / 50000\n",
      "Loss: 22.858630657196045, Accuracy: 0.078125\n",
      "Batch 704 / 50000\n",
      "Loss: 25.105536222457886, Accuracy: 0.234375\n",
      "Batch 768 / 50000\n",
      "Loss: 27.34773564338684, Accuracy: 0.28125\n",
      "Batch 832 / 50000\n",
      "Loss: 29.565991640090942, Accuracy: 0.390625\n",
      "Batch 896 / 50000\n",
      "Loss: 31.720023155212402, Accuracy: 0.5\n",
      "Batch 960 / 50000\n",
      "Loss: 33.849610567092896, Accuracy: 0.453125\n",
      "Batch 1024 / 50000\n",
      "Loss: 35.98440337181091, Accuracy: 0.34375\n",
      "Batch 1088 / 50000\n",
      "Loss: 38.01699256896973, Accuracy: 0.359375\n",
      "Batch 1152 / 50000\n",
      "Loss: 39.961546659469604, Accuracy: 0.546875\n",
      "Batch 1216 / 50000\n",
      "Loss: 41.972240686416626, Accuracy: 0.40625\n",
      "Batch 1280 / 50000\n",
      "Loss: 43.8881311416626, Accuracy: 0.4375\n",
      "Batch 1344 / 50000\n",
      "Loss: 45.64831054210663, Accuracy: 0.515625\n",
      "Batch 1408 / 50000\n",
      "Loss: 47.285053968429565, Accuracy: 0.484375\n",
      "Batch 1472 / 50000\n",
      "Loss: 48.96625804901123, Accuracy: 0.46875\n",
      "Batch 1536 / 50000\n",
      "Loss: 50.406490206718445, Accuracy: 0.625\n",
      "Batch 1600 / 50000\n",
      "Loss: 51.655059695243835, Accuracy: 0.75\n",
      "Batch 1664 / 50000\n",
      "Loss: 53.00940752029419, Accuracy: 0.578125\n",
      "Batch 1728 / 50000\n",
      "Loss: 54.24057614803314, Accuracy: 0.546875\n",
      "Batch 1792 / 50000\n",
      "Loss: 55.513389110565186, Accuracy: 0.546875\n",
      "Batch 1856 / 50000\n",
      "Loss: 56.819738268852234, Accuracy: 0.53125\n",
      "Batch 1920 / 50000\n",
      "Loss: 57.76604741811752, Accuracy: 0.640625\n",
      "Batch 1984 / 50000\n",
      "Loss: 58.600767731666565, Accuracy: 0.765625\n",
      "Batch 2048 / 50000\n",
      "Loss: 59.68413162231445, Accuracy: 0.609375\n",
      "Batch 2112 / 50000\n",
      "Loss: 60.75837790966034, Accuracy: 0.640625\n",
      "Batch 2176 / 50000\n",
      "Loss: 61.43306744098663, Accuracy: 0.8125\n",
      "Batch 2240 / 50000\n",
      "Loss: 62.2248632311821, Accuracy: 0.796875\n",
      "Batch 2304 / 50000\n",
      "Loss: 63.129890978336334, Accuracy: 0.71875\n",
      "Batch 2368 / 50000\n",
      "Loss: 63.97333812713623, Accuracy: 0.6875\n",
      "Batch 2432 / 50000\n",
      "Loss: 64.68962925672531, Accuracy: 0.734375\n",
      "Batch 2496 / 50000\n",
      "Loss: 65.60954546928406, Accuracy: 0.65625\n",
      "Batch 2560 / 50000\n",
      "Loss: 66.58608365058899, Accuracy: 0.671875\n",
      "Batch 2624 / 50000\n",
      "Loss: 67.52431565523148, Accuracy: 0.65625\n",
      "Batch 2688 / 50000\n",
      "Loss: 68.36171013116837, Accuracy: 0.75\n",
      "Batch 2752 / 50000\n",
      "Loss: 69.4176669716835, Accuracy: 0.65625\n",
      "Batch 2816 / 50000\n",
      "Loss: 70.31753718852997, Accuracy: 0.765625\n",
      "Batch 2880 / 50000\n",
      "Loss: 70.83870297670364, Accuracy: 0.828125\n",
      "Batch 2944 / 50000\n",
      "Loss: 71.62155210971832, Accuracy: 0.78125\n",
      "Batch 3008 / 50000\n",
      "Loss: 72.47450572252274, Accuracy: 0.796875\n",
      "Batch 3072 / 50000\n",
      "Loss: 73.05053180456161, Accuracy: 0.796875\n",
      "Batch 3136 / 50000\n",
      "Loss: 73.89070391654968, Accuracy: 0.765625\n",
      "Batch 3200 / 50000\n",
      "Loss: 74.63137036561966, Accuracy: 0.765625\n",
      "Batch 3264 / 50000\n",
      "Loss: 75.38330709934235, Accuracy: 0.78125\n",
      "Batch 3328 / 50000\n",
      "Loss: 76.02205383777618, Accuracy: 0.828125\n",
      "Batch 3392 / 50000\n",
      "Loss: 76.73482239246368, Accuracy: 0.828125\n",
      "Batch 3456 / 50000\n",
      "Loss: 77.4282574057579, Accuracy: 0.78125\n",
      "Batch 3520 / 50000\n",
      "Loss: 77.94752585887909, Accuracy: 0.828125\n",
      "Batch 3584 / 50000\n",
      "Loss: 78.48898911476135, Accuracy: 0.828125\n",
      "Batch 3648 / 50000\n",
      "Loss: 79.37110304832458, Accuracy: 0.71875\n",
      "Batch 3712 / 50000\n",
      "Loss: 79.88106369972229, Accuracy: 0.828125\n",
      "Batch 3776 / 50000\n",
      "Loss: 80.3400546014309, Accuracy: 0.84375\n",
      "Batch 3840 / 50000\n",
      "Loss: 80.76211285591125, Accuracy: 0.859375\n",
      "Batch 3904 / 50000\n",
      "Loss: 81.39776265621185, Accuracy: 0.84375\n",
      "Batch 3968 / 50000\n",
      "Loss: 81.80753916501999, Accuracy: 0.828125\n",
      "Batch 4032 / 50000\n",
      "Loss: 82.43502289056778, Accuracy: 0.765625\n",
      "Batch 4096 / 50000\n",
      "Loss: 83.0675117969513, Accuracy: 0.765625\n",
      "Batch 4160 / 50000\n",
      "Loss: 83.66859501600266, Accuracy: 0.828125\n",
      "Batch 4224 / 50000\n",
      "Loss: 84.02074500918388, Accuracy: 0.921875\n",
      "Batch 4288 / 50000\n",
      "Loss: 84.56392166018486, Accuracy: 0.828125\n",
      "Batch 4352 / 50000\n",
      "Loss: 85.06496378779411, Accuracy: 0.84375\n",
      "Batch 4416 / 50000\n",
      "Loss: 85.48521789908409, Accuracy: 0.890625\n",
      "Batch 4480 / 50000\n",
      "Loss: 86.29450038075447, Accuracy: 0.765625\n",
      "Batch 4544 / 50000\n",
      "Loss: 86.95945754647255, Accuracy: 0.75\n",
      "Batch 4608 / 50000\n",
      "Loss: 87.31396412849426, Accuracy: 0.921875\n",
      "Batch 4672 / 50000\n",
      "Loss: 87.54884262382984, Accuracy: 0.921875\n",
      "Batch 4736 / 50000\n",
      "Loss: 88.00952242314816, Accuracy: 0.859375\n",
      "Batch 4800 / 50000\n",
      "Loss: 88.46862597763538, Accuracy: 0.84375\n",
      "Batch 4864 / 50000\n",
      "Loss: 88.97522594034672, Accuracy: 0.84375\n",
      "Batch 4928 / 50000\n",
      "Loss: 89.60998709499836, Accuracy: 0.828125\n",
      "Batch 4992 / 50000\n",
      "Loss: 90.22897918522358, Accuracy: 0.796875\n",
      "Batch 5056 / 50000\n",
      "Loss: 90.72556518018246, Accuracy: 0.84375\n",
      "Batch 5120 / 50000\n",
      "Loss: 91.11362065374851, Accuracy: 0.921875\n",
      "Batch 5184 / 50000\n",
      "Loss: 91.45482571423054, Accuracy: 0.921875\n",
      "Batch 5248 / 50000\n",
      "Loss: 91.95882613956928, Accuracy: 0.828125\n",
      "Batch 5312 / 50000\n",
      "Loss: 92.37169222533703, Accuracy: 0.890625\n",
      "Batch 5376 / 50000\n",
      "Loss: 92.66919942200184, Accuracy: 0.90625\n",
      "Batch 5440 / 50000\n",
      "Loss: 92.87304443120956, Accuracy: 0.96875\n",
      "Batch 5504 / 50000\n",
      "Loss: 93.36352413892746, Accuracy: 0.875\n",
      "Batch 5568 / 50000\n",
      "Loss: 93.68463093042374, Accuracy: 0.921875\n",
      "Batch 5632 / 50000\n",
      "Loss: 94.03945615887642, Accuracy: 0.890625\n",
      "Batch 5696 / 50000\n",
      "Loss: 94.48679599165916, Accuracy: 0.875\n",
      "Batch 5760 / 50000\n",
      "Loss: 94.79587638378143, Accuracy: 0.9375\n",
      "Batch 5824 / 50000\n",
      "Loss: 95.13672089576721, Accuracy: 0.953125\n",
      "Batch 5888 / 50000\n",
      "Loss: 95.54531309008598, Accuracy: 0.796875\n",
      "Batch 5952 / 50000\n",
      "Loss: 95.79344116151333, Accuracy: 0.9375\n",
      "Batch 6016 / 50000\n",
      "Loss: 96.32977299392223, Accuracy: 0.84375\n",
      "Batch 6080 / 50000\n",
      "Loss: 96.56822794675827, Accuracy: 0.875\n",
      "Batch 6144 / 50000\n",
      "Loss: 97.03746727108955, Accuracy: 0.859375\n",
      "Batch 6208 / 50000\n",
      "Loss: 97.33124655485153, Accuracy: 0.90625\n",
      "Batch 6272 / 50000\n",
      "Loss: 97.88070929050446, Accuracy: 0.8125\n",
      "Batch 6336 / 50000\n",
      "Loss: 98.26343542337418, Accuracy: 0.875\n",
      "Batch 6400 / 50000\n",
      "Loss: 98.87531739473343, Accuracy: 0.8125\n",
      "Batch 6464 / 50000\n",
      "Loss: 99.29105165600777, Accuracy: 0.859375\n",
      "Batch 6528 / 50000\n",
      "Loss: 99.69552707672119, Accuracy: 0.890625\n",
      "Batch 6592 / 50000\n",
      "Loss: 100.099016726017, Accuracy: 0.890625\n",
      "Batch 6656 / 50000\n",
      "Loss: 100.61462634801865, Accuracy: 0.859375\n",
      "Batch 6720 / 50000\n",
      "Loss: 100.89908564090729, Accuracy: 0.921875\n",
      "Batch 6784 / 50000\n",
      "Loss: 101.25029852986336, Accuracy: 0.921875\n",
      "Batch 6848 / 50000\n",
      "Loss: 101.66501849889755, Accuracy: 0.859375\n",
      "Batch 6912 / 50000\n",
      "Loss: 102.25074577331543, Accuracy: 0.828125\n",
      "Batch 6976 / 50000\n",
      "Loss: 102.49665738642216, Accuracy: 0.953125\n",
      "Batch 7040 / 50000\n",
      "Loss: 102.94252662360668, Accuracy: 0.875\n",
      "Batch 7104 / 50000\n",
      "Loss: 103.1712078154087, Accuracy: 0.921875\n",
      "Batch 7168 / 50000\n",
      "Loss: 103.41935712099075, Accuracy: 0.953125\n",
      "Batch 7232 / 50000\n",
      "Loss: 103.89563205838203, Accuracy: 0.875\n",
      "Batch 7296 / 50000\n",
      "Loss: 104.29034107923508, Accuracy: 0.90625\n",
      "Batch 7360 / 50000\n",
      "Loss: 104.56025207042694, Accuracy: 0.90625\n",
      "Batch 7424 / 50000\n",
      "Loss: 104.76541271805763, Accuracy: 0.953125\n",
      "Batch 7488 / 50000\n",
      "Loss: 104.98480889201164, Accuracy: 0.921875\n",
      "Batch 7552 / 50000\n",
      "Loss: 105.25341737270355, Accuracy: 0.90625\n",
      "Batch 7616 / 50000\n",
      "Loss: 105.53570356965065, Accuracy: 0.921875\n",
      "Batch 7680 / 50000\n",
      "Loss: 105.64705616235733, Accuracy: 0.984375\n",
      "Batch 7744 / 50000\n",
      "Loss: 105.94461059570312, Accuracy: 0.9375\n",
      "Batch 7808 / 50000\n",
      "Loss: 106.07363951206207, Accuracy: 0.984375\n",
      "Batch 7872 / 50000\n",
      "Loss: 106.19142533093691, Accuracy: 0.984375\n",
      "Batch 7936 / 50000\n",
      "Loss: 106.40481338649988, Accuracy: 0.890625\n",
      "Batch 8000 / 50000\n",
      "Loss: 106.65300472825766, Accuracy: 0.890625\n",
      "Batch 8064 / 50000\n",
      "Loss: 106.8850621804595, Accuracy: 0.90625\n",
      "Batch 8128 / 50000\n",
      "Loss: 107.17012693732977, Accuracy: 0.890625\n",
      "Batch 8192 / 50000\n",
      "Loss: 107.45493584126234, Accuracy: 0.921875\n",
      "Batch 8256 / 50000\n",
      "Loss: 107.64116410166025, Accuracy: 0.90625\n",
      "Batch 8320 / 50000\n",
      "Loss: 107.74389988929033, Accuracy: 0.96875\n",
      "Batch 8384 / 50000\n",
      "Loss: 107.97082430869341, Accuracy: 0.90625\n",
      "Batch 8448 / 50000\n",
      "Loss: 108.14829217642546, Accuracy: 0.953125\n",
      "Batch 8512 / 50000\n",
      "Loss: 108.45099272578955, Accuracy: 0.90625\n",
      "Batch 8576 / 50000\n",
      "Loss: 108.6062755510211, Accuracy: 0.96875\n",
      "Batch 8640 / 50000\n",
      "Loss: 108.83903186768293, Accuracy: 0.96875\n",
      "Batch 8704 / 50000\n",
      "Loss: 109.03670593351126, Accuracy: 0.921875\n",
      "Batch 8768 / 50000\n",
      "Loss: 109.22771864384413, Accuracy: 0.953125\n",
      "Batch 8832 / 50000\n",
      "Loss: 109.501819036901, Accuracy: 0.9375\n",
      "Batch 8896 / 50000\n",
      "Loss: 109.58560585975647, Accuracy: 0.984375\n",
      "Batch 8960 / 50000\n",
      "Loss: 109.78032037615776, Accuracy: 0.9375\n",
      "Batch 9024 / 50000\n",
      "Loss: 110.07038632035255, Accuracy: 0.890625\n",
      "Batch 9088 / 50000\n",
      "Loss: 110.49845987558365, Accuracy: 0.84375\n",
      "Batch 9152 / 50000\n",
      "Loss: 110.6836119890213, Accuracy: 0.953125\n",
      "Batch 9216 / 50000\n",
      "Loss: 110.84198695421219, Accuracy: 0.96875\n",
      "Batch 9280 / 50000\n",
      "Loss: 111.00583630800247, Accuracy: 0.921875\n",
      "Batch 9344 / 50000\n",
      "Loss: 111.21013058722019, Accuracy: 0.9375\n",
      "Batch 9408 / 50000\n",
      "Loss: 111.3306717351079, Accuracy: 0.984375\n",
      "Batch 9472 / 50000\n",
      "Loss: 111.57472325116396, Accuracy: 0.921875\n",
      "Batch 9536 / 50000\n",
      "Loss: 111.78868941217661, Accuracy: 0.90625\n",
      "Batch 9600 / 50000\n",
      "Loss: 111.90979515016079, Accuracy: 0.984375\n",
      "Batch 9664 / 50000\n",
      "Loss: 112.18169705569744, Accuracy: 0.921875\n",
      "Batch 9728 / 50000\n",
      "Loss: 112.40141446888447, Accuracy: 0.921875\n",
      "Batch 9792 / 50000\n",
      "Loss: 112.4901283532381, Accuracy: 0.984375\n",
      "Batch 9856 / 50000\n",
      "Loss: 112.81319607794285, Accuracy: 0.921875\n",
      "Batch 9920 / 50000\n",
      "Loss: 112.98226736485958, Accuracy: 0.953125\n",
      "Batch 9984 / 50000\n",
      "Loss: 113.08607310056686, Accuracy: 0.96875\n",
      "Batch 10048 / 50000\n",
      "Loss: 113.24345362186432, Accuracy: 0.9375\n",
      "Batch 10112 / 50000\n",
      "Loss: 113.51142427325249, Accuracy: 0.90625\n",
      "Batch 10176 / 50000\n",
      "Loss: 113.6740815192461, Accuracy: 0.953125\n",
      "Batch 10240 / 50000\n",
      "Loss: 113.86385814845562, Accuracy: 0.921875\n",
      "Batch 10304 / 50000\n",
      "Loss: 114.04226064682007, Accuracy: 0.9375\n",
      "Batch 10368 / 50000\n",
      "Loss: 114.31499552726746, Accuracy: 0.921875\n",
      "Batch 10432 / 50000\n",
      "Loss: 114.62678742408752, Accuracy: 0.90625\n",
      "Batch 10496 / 50000\n",
      "Loss: 114.83674055337906, Accuracy: 0.9375\n",
      "Batch 10560 / 50000\n",
      "Loss: 115.26634404063225, Accuracy: 0.921875\n",
      "Batch 10624 / 50000\n",
      "Loss: 115.355987906456, Accuracy: 0.96875\n",
      "Batch 10688 / 50000\n",
      "Loss: 115.67041039466858, Accuracy: 0.90625\n",
      "Batch 10752 / 50000\n",
      "Loss: 115.81467005610466, Accuracy: 0.953125\n",
      "Batch 10816 / 50000\n",
      "Loss: 116.10892352461815, Accuracy: 0.890625\n",
      "Batch 10880 / 50000\n",
      "Loss: 116.30338715016842, Accuracy: 0.921875\n",
      "Batch 10944 / 50000\n",
      "Loss: 116.58978046476841, Accuracy: 0.953125\n",
      "Batch 11008 / 50000\n",
      "Loss: 116.76838105916977, Accuracy: 0.953125\n",
      "Batch 11072 / 50000\n",
      "Loss: 117.06408721208572, Accuracy: 0.921875\n",
      "Batch 11136 / 50000\n",
      "Loss: 117.3123569637537, Accuracy: 0.9375\n",
      "Batch 11200 / 50000\n",
      "Loss: 117.4737971574068, Accuracy: 0.953125\n",
      "Batch 11264 / 50000\n",
      "Loss: 117.77330757677555, Accuracy: 0.90625\n",
      "Batch 11328 / 50000\n",
      "Loss: 118.02075357735157, Accuracy: 0.9375\n",
      "Batch 11392 / 50000\n",
      "Loss: 118.17990274727345, Accuracy: 0.953125\n",
      "Batch 11456 / 50000\n",
      "Loss: 118.37548138201237, Accuracy: 0.921875\n",
      "Batch 11520 / 50000\n",
      "Loss: 118.60958835482597, Accuracy: 0.90625\n",
      "Batch 11584 / 50000\n",
      "Loss: 118.73409160226583, Accuracy: 0.96875\n",
      "Batch 11648 / 50000\n",
      "Loss: 118.93582401424646, Accuracy: 0.96875\n",
      "Batch 11712 / 50000\n",
      "Loss: 119.22065822035074, Accuracy: 0.921875\n",
      "Batch 11776 / 50000\n",
      "Loss: 119.37864433974028, Accuracy: 0.90625\n",
      "Batch 11840 / 50000\n",
      "Loss: 119.43506856262684, Accuracy: 0.984375\n",
      "Batch 11904 / 50000\n",
      "Loss: 119.69058476388454, Accuracy: 0.921875\n",
      "Batch 11968 / 50000\n",
      "Loss: 119.96956838667393, Accuracy: 0.875\n",
      "Batch 12032 / 50000\n",
      "Loss: 120.31239058077335, Accuracy: 0.875\n",
      "Batch 12096 / 50000\n",
      "Loss: 120.72400398552418, Accuracy: 0.921875\n",
      "Batch 12160 / 50000\n",
      "Loss: 120.91740269958973, Accuracy: 0.921875\n",
      "Batch 12224 / 50000\n",
      "Loss: 121.11301816999912, Accuracy: 0.953125\n",
      "Batch 12288 / 50000\n",
      "Loss: 121.31060384213924, Accuracy: 0.9375\n",
      "Batch 12352 / 50000\n",
      "Loss: 121.4254013672471, Accuracy: 0.953125\n",
      "Batch 12416 / 50000\n",
      "Loss: 121.62253236025572, Accuracy: 0.9375\n",
      "Batch 12480 / 50000\n",
      "Loss: 121.7866682484746, Accuracy: 0.96875\n",
      "Batch 12544 / 50000\n",
      "Loss: 121.99921465665102, Accuracy: 0.921875\n",
      "Batch 12608 / 50000\n",
      "Loss: 122.26042867451906, Accuracy: 0.921875\n",
      "Batch 12672 / 50000\n",
      "Loss: 122.38767705112696, Accuracy: 0.96875\n",
      "Batch 12736 / 50000\n",
      "Loss: 122.5051459595561, Accuracy: 0.96875\n",
      "Batch 12800 / 50000\n",
      "Loss: 122.60229142010212, Accuracy: 0.96875\n",
      "Batch 12864 / 50000\n",
      "Loss: 122.8137911260128, Accuracy: 0.9375\n",
      "Batch 12928 / 50000\n",
      "Loss: 122.96974624693394, Accuracy: 0.9375\n",
      "Batch 12992 / 50000\n",
      "Loss: 123.13103960454464, Accuracy: 0.96875\n",
      "Batch 13056 / 50000\n",
      "Loss: 123.33628414571285, Accuracy: 0.90625\n",
      "Batch 13120 / 50000\n",
      "Loss: 123.43450453132391, Accuracy: 0.984375\n",
      "Batch 13184 / 50000\n",
      "Loss: 123.62498823553324, Accuracy: 0.921875\n",
      "Batch 13248 / 50000\n",
      "Loss: 123.79645816236734, Accuracy: 0.953125\n",
      "Batch 13312 / 50000\n",
      "Loss: 124.00816292315722, Accuracy: 0.921875\n",
      "Batch 13376 / 50000\n",
      "Loss: 124.14197962731123, Accuracy: 0.953125\n",
      "Batch 13440 / 50000\n",
      "Loss: 124.21510630846024, Accuracy: 0.96875\n",
      "Batch 13504 / 50000\n",
      "Loss: 124.34285388886929, Accuracy: 0.96875\n",
      "Batch 13568 / 50000\n",
      "Loss: 124.45572072267532, Accuracy: 0.953125\n",
      "Batch 13632 / 50000\n",
      "Loss: 124.50909600779414, Accuracy: 0.984375\n",
      "Batch 13696 / 50000\n",
      "Loss: 124.65254632756114, Accuracy: 0.953125\n",
      "Batch 13760 / 50000\n",
      "Loss: 124.91587368771434, Accuracy: 0.921875\n",
      "Batch 13824 / 50000\n",
      "Loss: 125.07302422448993, Accuracy: 0.9375\n",
      "Batch 13888 / 50000\n",
      "Loss: 125.22104744240642, Accuracy: 0.953125\n",
      "Batch 13952 / 50000\n",
      "Loss: 125.3664087317884, Accuracy: 0.953125\n",
      "Batch 14016 / 50000\n",
      "Loss: 125.45851308479905, Accuracy: 0.96875\n",
      "Batch 14080 / 50000\n",
      "Loss: 125.59878315404058, Accuracy: 0.9375\n",
      "Batch 14144 / 50000\n",
      "Loss: 125.72605961933732, Accuracy: 0.9375\n",
      "Batch 14208 / 50000\n",
      "Loss: 125.7950400672853, Accuracy: 0.96875\n",
      "Batch 14272 / 50000\n",
      "Loss: 126.0382720194757, Accuracy: 0.953125\n",
      "Batch 14336 / 50000\n",
      "Loss: 126.22581633552909, Accuracy: 0.921875\n",
      "Batch 14400 / 50000\n",
      "Loss: 126.34133284911513, Accuracy: 0.953125\n",
      "Batch 14464 / 50000\n",
      "Loss: 126.45304913446307, Accuracy: 0.953125\n",
      "Batch 14528 / 50000\n",
      "Loss: 126.50041404739022, Accuracy: 0.984375\n",
      "Batch 14592 / 50000\n",
      "Loss: 126.75450680032372, Accuracy: 0.921875\n",
      "Batch 14656 / 50000\n",
      "Loss: 126.97232334688306, Accuracy: 0.921875\n",
      "Batch 14720 / 50000\n",
      "Loss: 127.1016893722117, Accuracy: 0.9375\n",
      "Batch 14784 / 50000\n",
      "Loss: 127.14705222472548, Accuracy: 1.0\n",
      "Batch 14848 / 50000\n",
      "Loss: 127.44713908061385, Accuracy: 0.890625\n",
      "Batch 14912 / 50000\n",
      "Loss: 127.70924538001418, Accuracy: 0.90625\n",
      "Batch 14976 / 50000\n",
      "Loss: 127.8939401768148, Accuracy: 0.921875\n",
      "Batch 15040 / 50000\n",
      "Loss: 128.01759516820312, Accuracy: 0.953125\n",
      "Batch 15104 / 50000\n",
      "Loss: 128.29713263735175, Accuracy: 0.90625\n",
      "Batch 15168 / 50000\n",
      "Loss: 128.39927363023162, Accuracy: 0.96875\n",
      "Batch 15232 / 50000\n",
      "Loss: 128.496475931257, Accuracy: 0.96875\n",
      "Batch 15296 / 50000\n",
      "Loss: 128.61863330379128, Accuracy: 0.96875\n",
      "Batch 15360 / 50000\n",
      "Loss: 128.74815710261464, Accuracy: 0.953125\n",
      "Batch 15424 / 50000\n",
      "Loss: 128.9743948765099, Accuracy: 0.953125\n",
      "Batch 15488 / 50000\n",
      "Loss: 129.1162568219006, Accuracy: 0.96875\n",
      "Batch 15552 / 50000\n",
      "Loss: 129.1939113251865, Accuracy: 0.984375\n",
      "Batch 15616 / 50000\n",
      "Loss: 129.4268711619079, Accuracy: 0.921875\n",
      "Batch 15680 / 50000\n",
      "Loss: 129.65898023173213, Accuracy: 0.9375\n",
      "Batch 15744 / 50000\n",
      "Loss: 129.72439851984382, Accuracy: 0.984375\n",
      "Batch 15808 / 50000\n",
      "Loss: 129.82537715509534, Accuracy: 0.96875\n",
      "Batch 15872 / 50000\n",
      "Loss: 129.93629090860486, Accuracy: 0.96875\n",
      "Batch 15936 / 50000\n",
      "Loss: 130.05872797593474, Accuracy: 0.953125\n",
      "Batch 16000 / 50000\n",
      "Loss: 130.1826785467565, Accuracy: 0.953125\n",
      "Batch 16064 / 50000\n",
      "Loss: 130.34924764558673, Accuracy: 0.953125\n",
      "Batch 16128 / 50000\n",
      "Loss: 130.5721816830337, Accuracy: 0.921875\n",
      "Batch 16192 / 50000\n",
      "Loss: 130.85216687247157, Accuracy: 0.921875\n",
      "Batch 16256 / 50000\n",
      "Loss: 130.9265366010368, Accuracy: 0.96875\n",
      "Batch 16320 / 50000\n",
      "Loss: 131.1714806817472, Accuracy: 0.9375\n",
      "Batch 16384 / 50000\n",
      "Loss: 131.2949557043612, Accuracy: 0.96875\n",
      "Batch 16448 / 50000\n",
      "Loss: 131.41063426062465, Accuracy: 0.96875\n",
      "Batch 16512 / 50000\n",
      "Loss: 131.47440667822957, Accuracy: 0.96875\n",
      "Batch 16576 / 50000\n",
      "Loss: 131.59840048477054, Accuracy: 0.9375\n",
      "Batch 16640 / 50000\n",
      "Loss: 131.72828113660216, Accuracy: 0.9375\n",
      "Batch 16704 / 50000\n",
      "Loss: 131.82264664396644, Accuracy: 0.984375\n",
      "Batch 16768 / 50000\n",
      "Loss: 131.9199420697987, Accuracy: 0.9375\n",
      "Batch 16832 / 50000\n",
      "Loss: 132.0542092360556, Accuracy: 0.953125\n",
      "Batch 16896 / 50000\n",
      "Loss: 132.1438136063516, Accuracy: 0.96875\n",
      "Batch 16960 / 50000\n",
      "Loss: 132.42315914854407, Accuracy: 0.90625\n",
      "Batch 17024 / 50000\n",
      "Loss: 132.55207144841552, Accuracy: 0.96875\n",
      "Batch 17088 / 50000\n",
      "Loss: 132.64040261134505, Accuracy: 0.984375\n",
      "Batch 17152 / 50000\n",
      "Loss: 132.67770087718964, Accuracy: 0.984375\n",
      "Batch 17216 / 50000\n",
      "Loss: 132.83334396779537, Accuracy: 0.953125\n",
      "Batch 17280 / 50000\n",
      "Loss: 133.203336045146, Accuracy: 0.90625\n",
      "Batch 17344 / 50000\n",
      "Loss: 133.40484757721424, Accuracy: 0.90625\n",
      "Batch 17408 / 50000\n",
      "Loss: 133.6489311903715, Accuracy: 0.953125\n",
      "Batch 17472 / 50000\n",
      "Loss: 133.77053982019424, Accuracy: 0.96875\n",
      "Batch 17536 / 50000\n",
      "Loss: 133.83471009880304, Accuracy: 0.984375\n",
      "Batch 17600 / 50000\n",
      "Loss: 134.05541672557592, Accuracy: 0.9375\n",
      "Batch 17664 / 50000\n",
      "Loss: 134.1011820808053, Accuracy: 0.984375\n",
      "Batch 17728 / 50000\n",
      "Loss: 134.37559447437525, Accuracy: 0.953125\n",
      "Batch 17792 / 50000\n",
      "Loss: 134.49183113873005, Accuracy: 0.953125\n",
      "Batch 17856 / 50000\n",
      "Loss: 134.6663129478693, Accuracy: 0.96875\n",
      "Batch 17920 / 50000\n",
      "Loss: 134.7656188607216, Accuracy: 0.96875\n",
      "Batch 17984 / 50000\n",
      "Loss: 135.00393868982792, Accuracy: 0.9375\n",
      "Batch 18048 / 50000\n",
      "Loss: 135.1954267323017, Accuracy: 0.9375\n",
      "Batch 18112 / 50000\n",
      "Loss: 135.26357297599316, Accuracy: 0.96875\n",
      "Batch 18176 / 50000\n",
      "Loss: 135.33065672963858, Accuracy: 0.984375\n",
      "Batch 18240 / 50000\n",
      "Loss: 135.56400498002768, Accuracy: 0.90625\n",
      "Batch 18304 / 50000\n",
      "Loss: 135.6837084516883, Accuracy: 0.953125\n",
      "Batch 18368 / 50000\n",
      "Loss: 135.77501914650202, Accuracy: 0.96875\n",
      "Batch 18432 / 50000\n",
      "Loss: 136.0601550117135, Accuracy: 0.953125\n",
      "Batch 18496 / 50000\n",
      "Loss: 136.22540483623743, Accuracy: 0.96875\n",
      "Batch 18560 / 50000\n",
      "Loss: 136.30519925057888, Accuracy: 0.96875\n",
      "Batch 18624 / 50000\n",
      "Loss: 136.42551947385073, Accuracy: 0.9375\n",
      "Batch 18688 / 50000\n",
      "Loss: 136.6751630231738, Accuracy: 0.921875\n",
      "Batch 18752 / 50000\n",
      "Loss: 136.7373236902058, Accuracy: 1.0\n",
      "Batch 18816 / 50000\n",
      "Loss: 136.80600472167134, Accuracy: 0.984375\n",
      "Batch 18880 / 50000\n",
      "Loss: 136.94225696101785, Accuracy: 0.921875\n",
      "Batch 18944 / 50000\n",
      "Loss: 136.96212858520448, Accuracy: 1.0\n",
      "Batch 19008 / 50000\n",
      "Loss: 136.9881085269153, Accuracy: 1.0\n",
      "Batch 19072 / 50000\n",
      "Loss: 137.07641429826617, Accuracy: 0.984375\n",
      "Batch 19136 / 50000\n",
      "Loss: 137.12590031325817, Accuracy: 1.0\n",
      "Batch 19200 / 50000\n",
      "Loss: 137.42749167978764, Accuracy: 0.90625\n",
      "Batch 19264 / 50000\n",
      "Loss: 137.6224188953638, Accuracy: 0.953125\n",
      "Batch 19328 / 50000\n",
      "Loss: 137.7318566069007, Accuracy: 0.96875\n",
      "Batch 19392 / 50000\n",
      "Loss: 137.7991181910038, Accuracy: 0.96875\n",
      "Batch 19456 / 50000\n",
      "Loss: 137.90578193217516, Accuracy: 0.953125\n",
      "Batch 19520 / 50000\n",
      "Loss: 137.9775078818202, Accuracy: 1.0\n",
      "Batch 19584 / 50000\n",
      "Loss: 138.1749603971839, Accuracy: 0.90625\n",
      "Batch 19648 / 50000\n",
      "Loss: 138.27621984481812, Accuracy: 0.953125\n",
      "Batch 19712 / 50000\n",
      "Loss: 138.38314060121775, Accuracy: 0.953125\n",
      "Batch 19776 / 50000\n",
      "Loss: 138.6105176433921, Accuracy: 0.90625\n",
      "Batch 19840 / 50000\n",
      "Loss: 138.70722842216492, Accuracy: 0.984375\n",
      "Batch 19904 / 50000\n",
      "Loss: 138.86727610230446, Accuracy: 0.953125\n",
      "Batch 19968 / 50000\n",
      "Loss: 139.03627753257751, Accuracy: 0.96875\n",
      "Batch 20032 / 50000\n",
      "Loss: 139.1502725854516, Accuracy: 0.96875\n",
      "Batch 20096 / 50000\n",
      "Loss: 139.24102897942066, Accuracy: 0.984375\n",
      "Batch 20160 / 50000\n",
      "Loss: 139.35039397329092, Accuracy: 0.953125\n",
      "Batch 20224 / 50000\n",
      "Loss: 139.3815424181521, Accuracy: 1.0\n",
      "Batch 20288 / 50000\n",
      "Loss: 139.55606052652, Accuracy: 0.953125\n",
      "Batch 20352 / 50000\n",
      "Loss: 139.6949359215796, Accuracy: 0.953125\n",
      "Batch 20416 / 50000\n",
      "Loss: 139.92484647408128, Accuracy: 0.90625\n",
      "Batch 20480 / 50000\n",
      "Loss: 139.9647914133966, Accuracy: 1.0\n",
      "Batch 20544 / 50000\n",
      "Loss: 140.18156802281737, Accuracy: 0.9375\n",
      "Batch 20608 / 50000\n",
      "Loss: 140.26708966866136, Accuracy: 0.96875\n",
      "Batch 20672 / 50000\n",
      "Loss: 140.36837198212743, Accuracy: 0.96875\n",
      "Batch 20736 / 50000\n",
      "Loss: 140.44754614308476, Accuracy: 0.984375\n",
      "Batch 20800 / 50000\n",
      "Loss: 140.5175511725247, Accuracy: 0.984375\n",
      "Batch 20864 / 50000\n",
      "Loss: 140.61661592498422, Accuracy: 0.9375\n",
      "Batch 20928 / 50000\n",
      "Loss: 140.70211021229625, Accuracy: 0.953125\n",
      "Batch 20992 / 50000\n",
      "Loss: 140.80070517584682, Accuracy: 0.953125\n",
      "Batch 21056 / 50000\n",
      "Loss: 140.84266367927194, Accuracy: 1.0\n",
      "Batch 21120 / 50000\n",
      "Loss: 140.95904346928, Accuracy: 0.9375\n",
      "Batch 21184 / 50000\n",
      "Loss: 141.1216189675033, Accuracy: 0.96875\n",
      "Batch 21248 / 50000\n",
      "Loss: 141.2499486617744, Accuracy: 0.9375\n",
      "Batch 21312 / 50000\n",
      "Loss: 141.32834715768695, Accuracy: 0.96875\n",
      "Batch 21376 / 50000\n",
      "Loss: 141.50624689087272, Accuracy: 0.953125\n",
      "Batch 21440 / 50000\n",
      "Loss: 141.64848243817687, Accuracy: 0.90625\n",
      "Batch 21504 / 50000\n",
      "Loss: 141.8294747583568, Accuracy: 0.96875\n",
      "Batch 21568 / 50000\n",
      "Loss: 142.06659410521388, Accuracy: 0.921875\n",
      "Batch 21632 / 50000\n",
      "Loss: 142.19642528519034, Accuracy: 0.953125\n",
      "Batch 21696 / 50000\n",
      "Loss: 142.37910526618361, Accuracy: 0.96875\n",
      "Batch 21760 / 50000\n",
      "Loss: 142.50611658021808, Accuracy: 0.984375\n",
      "Batch 21824 / 50000\n",
      "Loss: 142.64818669483066, Accuracy: 0.953125\n",
      "Batch 21888 / 50000\n",
      "Loss: 142.73645997419953, Accuracy: 0.984375\n",
      "Batch 21952 / 50000\n",
      "Loss: 142.85837214812636, Accuracy: 0.96875\n",
      "Batch 22016 / 50000\n",
      "Loss: 142.87591033615172, Accuracy: 1.0\n",
      "Batch 22080 / 50000\n",
      "Loss: 143.03669909574091, Accuracy: 0.921875\n",
      "Batch 22144 / 50000\n",
      "Loss: 143.12298264540732, Accuracy: 0.953125\n",
      "Batch 22208 / 50000\n",
      "Loss: 143.16820774041116, Accuracy: 0.984375\n",
      "Batch 22272 / 50000\n",
      "Loss: 143.2285877559334, Accuracy: 0.984375\n",
      "Batch 22336 / 50000\n",
      "Loss: 143.27452867664397, Accuracy: 0.984375\n",
      "Batch 22400 / 50000\n",
      "Loss: 143.39145326055586, Accuracy: 0.96875\n",
      "Batch 22464 / 50000\n",
      "Loss: 143.5073103774339, Accuracy: 0.953125\n",
      "Batch 22528 / 50000\n",
      "Loss: 143.54903056658804, Accuracy: 1.0\n",
      "Batch 22592 / 50000\n",
      "Loss: 143.5794488657266, Accuracy: 1.0\n",
      "Batch 22656 / 50000\n",
      "Loss: 143.6538960505277, Accuracy: 0.96875\n",
      "Batch 22720 / 50000\n",
      "Loss: 143.79226926155388, Accuracy: 0.96875\n",
      "Batch 22784 / 50000\n",
      "Loss: 144.0225266236812, Accuracy: 0.90625\n",
      "Batch 22848 / 50000\n",
      "Loss: 144.23833792097867, Accuracy: 0.921875\n",
      "Batch 22912 / 50000\n",
      "Loss: 144.39921294339, Accuracy: 0.953125\n",
      "Batch 22976 / 50000\n",
      "Loss: 144.44034182466567, Accuracy: 1.0\n",
      "Batch 23040 / 50000\n",
      "Loss: 144.50107233785093, Accuracy: 0.96875\n",
      "Batch 23104 / 50000\n",
      "Loss: 144.64597678743303, Accuracy: 0.96875\n",
      "Batch 23168 / 50000\n",
      "Loss: 144.78560093604028, Accuracy: 0.9375\n",
      "Batch 23232 / 50000\n",
      "Loss: 144.82421441935003, Accuracy: 1.0\n",
      "Batch 23296 / 50000\n",
      "Loss: 145.02538028918207, Accuracy: 0.921875\n",
      "Batch 23360 / 50000\n",
      "Loss: 145.0806950200349, Accuracy: 0.984375\n",
      "Batch 23424 / 50000\n",
      "Loss: 145.2134078014642, Accuracy: 0.953125\n",
      "Batch 23488 / 50000\n",
      "Loss: 145.33692192472517, Accuracy: 0.9375\n",
      "Batch 23552 / 50000\n",
      "Loss: 145.4296553079039, Accuracy: 0.984375\n",
      "Batch 23616 / 50000\n",
      "Loss: 145.52238247729838, Accuracy: 0.96875\n",
      "Batch 23680 / 50000\n",
      "Loss: 145.58845010586083, Accuracy: 0.984375\n",
      "Batch 23744 / 50000\n",
      "Loss: 145.7022314015776, Accuracy: 0.96875\n",
      "Batch 23808 / 50000\n",
      "Loss: 145.7978616449982, Accuracy: 0.96875\n",
      "Batch 23872 / 50000\n",
      "Loss: 145.8774695713073, Accuracy: 0.96875\n",
      "Batch 23936 / 50000\n",
      "Loss: 145.98554399050772, Accuracy: 0.96875\n",
      "Batch 24000 / 50000\n",
      "Loss: 146.14450816251338, Accuracy: 0.96875\n",
      "Batch 24064 / 50000\n",
      "Loss: 146.26603402383626, Accuracy: 0.953125\n",
      "Batch 24128 / 50000\n",
      "Loss: 146.32901383377612, Accuracy: 0.984375\n",
      "Batch 24192 / 50000\n",
      "Loss: 146.3842436093837, Accuracy: 0.984375\n",
      "Batch 24256 / 50000\n",
      "Loss: 146.40941042266786, Accuracy: 1.0\n",
      "Batch 24320 / 50000\n",
      "Loss: 146.48141839168966, Accuracy: 0.984375\n",
      "Batch 24384 / 50000\n",
      "Loss: 146.55293639563024, Accuracy: 0.953125\n",
      "Batch 24448 / 50000\n",
      "Loss: 146.7269960101694, Accuracy: 0.953125\n",
      "Batch 24512 / 50000\n",
      "Loss: 146.80806093476713, Accuracy: 0.96875\n",
      "Batch 24576 / 50000\n",
      "Loss: 146.84072906710207, Accuracy: 1.0\n",
      "Batch 24640 / 50000\n",
      "Loss: 147.04720608331263, Accuracy: 0.921875\n",
      "Batch 24704 / 50000\n",
      "Loss: 147.17359488643706, Accuracy: 0.9375\n",
      "Batch 24768 / 50000\n",
      "Loss: 147.3236082587391, Accuracy: 0.953125\n",
      "Batch 24832 / 50000\n",
      "Loss: 147.4563742671162, Accuracy: 0.96875\n",
      "Batch 24896 / 50000\n",
      "Loss: 147.6210495773703, Accuracy: 0.9375\n",
      "Batch 24960 / 50000\n",
      "Loss: 147.76217124797404, Accuracy: 0.953125\n",
      "Batch 25024 / 50000\n",
      "Loss: 147.86544605530798, Accuracy: 0.96875\n",
      "Batch 25088 / 50000\n",
      "Loss: 148.02932667173445, Accuracy: 0.953125\n",
      "Batch 25152 / 50000\n",
      "Loss: 148.09090144746006, Accuracy: 0.984375\n",
      "Batch 25216 / 50000\n",
      "Loss: 148.15802450291812, Accuracy: 0.984375\n",
      "Batch 25280 / 50000\n",
      "Loss: 148.27194271050394, Accuracy: 0.953125\n",
      "Batch 25344 / 50000\n",
      "Loss: 148.46662837825716, Accuracy: 0.9375\n",
      "Batch 25408 / 50000\n",
      "Loss: 148.67537317238748, Accuracy: 0.96875\n",
      "Batch 25472 / 50000\n",
      "Loss: 148.83004506491125, Accuracy: 0.953125\n",
      "Batch 25536 / 50000\n",
      "Loss: 148.95296734385192, Accuracy: 0.953125\n",
      "Batch 25600 / 50000\n",
      "Loss: 149.05516553483903, Accuracy: 0.984375\n",
      "Batch 25664 / 50000\n",
      "Loss: 149.15873490087688, Accuracy: 0.984375\n",
      "Batch 25728 / 50000\n",
      "Loss: 149.25182860903442, Accuracy: 0.953125\n",
      "Batch 25792 / 50000\n",
      "Loss: 149.42918650619686, Accuracy: 0.9375\n",
      "Batch 25856 / 50000\n",
      "Loss: 149.48538388125598, Accuracy: 0.984375\n",
      "Batch 25920 / 50000\n",
      "Loss: 149.55008312873542, Accuracy: 0.984375\n",
      "Batch 25984 / 50000\n",
      "Loss: 149.80258875899017, Accuracy: 0.921875\n",
      "Batch 26048 / 50000\n",
      "Loss: 149.95899784378707, Accuracy: 0.9375\n",
      "Batch 26112 / 50000\n",
      "Loss: 150.06556155346334, Accuracy: 0.96875\n",
      "Batch 26176 / 50000\n",
      "Loss: 150.2931319978088, Accuracy: 0.90625\n",
      "Batch 26240 / 50000\n",
      "Loss: 150.36946650408208, Accuracy: 0.984375\n",
      "Batch 26304 / 50000\n",
      "Loss: 150.4630258809775, Accuracy: 0.984375\n",
      "Batch 26368 / 50000\n",
      "Loss: 150.51191204600036, Accuracy: 1.0\n",
      "Batch 26432 / 50000\n",
      "Loss: 150.57763065211475, Accuracy: 0.96875\n",
      "Batch 26496 / 50000\n",
      "Loss: 150.65173385851085, Accuracy: 0.984375\n",
      "Batch 26560 / 50000\n",
      "Loss: 150.7021801006049, Accuracy: 0.984375\n",
      "Batch 26624 / 50000\n",
      "Loss: 150.76525386236608, Accuracy: 0.984375\n",
      "Batch 26688 / 50000\n",
      "Loss: 150.83382725529373, Accuracy: 0.96875\n",
      "Batch 26752 / 50000\n",
      "Loss: 151.0961937289685, Accuracy: 0.9375\n",
      "Batch 26816 / 50000\n",
      "Loss: 151.17100269161165, Accuracy: 0.984375\n",
      "Batch 26880 / 50000\n",
      "Loss: 151.23125921748579, Accuracy: 0.984375\n",
      "Batch 26944 / 50000\n",
      "Loss: 151.3129433747381, Accuracy: 0.96875\n",
      "Batch 27008 / 50000\n",
      "Loss: 151.4105679821223, Accuracy: 0.96875\n",
      "Batch 27072 / 50000\n",
      "Loss: 151.44771338813007, Accuracy: 0.984375\n",
      "Batch 27136 / 50000\n",
      "Loss: 151.47193414717913, Accuracy: 1.0\n",
      "Batch 27200 / 50000\n",
      "Loss: 151.4869220610708, Accuracy: 1.0\n",
      "Batch 27264 / 50000\n",
      "Loss: 151.58884425275028, Accuracy: 0.953125\n",
      "Batch 27328 / 50000\n",
      "Loss: 151.62007097154856, Accuracy: 0.984375\n",
      "Batch 27392 / 50000\n",
      "Loss: 151.66312950849533, Accuracy: 0.984375\n",
      "Batch 27456 / 50000\n",
      "Loss: 151.68055721744895, Accuracy: 1.0\n",
      "Batch 27520 / 50000\n",
      "Loss: 151.80093612894416, Accuracy: 0.96875\n",
      "Batch 27584 / 50000\n",
      "Loss: 152.1055604480207, Accuracy: 0.90625\n",
      "Batch 27648 / 50000\n",
      "Loss: 152.13032867200673, Accuracy: 1.0\n",
      "Batch 27712 / 50000\n",
      "Loss: 152.1961327213794, Accuracy: 0.984375\n",
      "Batch 27776 / 50000\n",
      "Loss: 152.24691034667194, Accuracy: 0.984375\n",
      "Batch 27840 / 50000\n",
      "Loss: 152.26645909808576, Accuracy: 1.0\n",
      "Batch 27904 / 50000\n",
      "Loss: 152.39107913337648, Accuracy: 0.96875\n",
      "Batch 27968 / 50000\n",
      "Loss: 152.5480402018875, Accuracy: 0.953125\n",
      "Batch 28032 / 50000\n",
      "Loss: 152.59861147589982, Accuracy: 0.96875\n",
      "Batch 28096 / 50000\n",
      "Loss: 152.67187071032822, Accuracy: 0.96875\n",
      "Batch 28160 / 50000\n",
      "Loss: 152.8275251854211, Accuracy: 0.96875\n",
      "Batch 28224 / 50000\n",
      "Loss: 152.88224062882364, Accuracy: 0.984375\n",
      "Batch 28288 / 50000\n",
      "Loss: 153.03516787849367, Accuracy: 0.953125\n",
      "Batch 28352 / 50000\n",
      "Loss: 153.21741246245801, Accuracy: 0.9375\n",
      "Batch 28416 / 50000\n",
      "Loss: 153.26204589568079, Accuracy: 0.984375\n",
      "Batch 28480 / 50000\n",
      "Loss: 153.43790521286428, Accuracy: 0.953125\n",
      "Batch 28544 / 50000\n",
      "Loss: 153.48384935222566, Accuracy: 0.96875\n",
      "Batch 28608 / 50000\n",
      "Loss: 153.51532981730998, Accuracy: 1.0\n",
      "Batch 28672 / 50000\n",
      "Loss: 153.56883238069713, Accuracy: 0.984375\n",
      "Batch 28736 / 50000\n",
      "Loss: 153.6347051244229, Accuracy: 0.984375\n",
      "Batch 28800 / 50000\n",
      "Loss: 153.6793907713145, Accuracy: 0.984375\n",
      "Batch 28864 / 50000\n",
      "Loss: 153.71405925787985, Accuracy: 0.984375\n",
      "Batch 28928 / 50000\n",
      "Loss: 153.90154536999762, Accuracy: 0.9375\n",
      "Batch 28992 / 50000\n",
      "Loss: 153.93181187473238, Accuracy: 0.984375\n",
      "Batch 29056 / 50000\n",
      "Loss: 153.9860335905105, Accuracy: 0.984375\n",
      "Batch 29120 / 50000\n",
      "Loss: 154.0730188731104, Accuracy: 0.96875\n",
      "Batch 29184 / 50000\n",
      "Loss: 154.08841022010893, Accuracy: 1.0\n",
      "Batch 29248 / 50000\n",
      "Loss: 154.22522360738367, Accuracy: 0.953125\n",
      "Batch 29312 / 50000\n",
      "Loss: 154.31596878077835, Accuracy: 0.96875\n",
      "Batch 29376 / 50000\n",
      "Loss: 154.40507326181978, Accuracy: 0.96875\n",
      "Batch 29440 / 50000\n",
      "Loss: 154.5431928699836, Accuracy: 0.953125\n",
      "Batch 29504 / 50000\n",
      "Loss: 154.69280172046274, Accuracy: 0.953125\n",
      "Batch 29568 / 50000\n",
      "Loss: 154.86638196464628, Accuracy: 0.953125\n",
      "Batch 29632 / 50000\n",
      "Loss: 155.0591483777389, Accuracy: 0.96875\n",
      "Batch 29696 / 50000\n",
      "Loss: 155.12939965631813, Accuracy: 0.984375\n",
      "Batch 29760 / 50000\n",
      "Loss: 155.26324376370758, Accuracy: 0.9375\n",
      "Batch 29824 / 50000\n",
      "Loss: 155.29596017953008, Accuracy: 1.0\n",
      "Batch 29888 / 50000\n",
      "Loss: 155.41146255936474, Accuracy: 0.96875\n",
      "Batch 29952 / 50000\n",
      "Loss: 155.43754838313907, Accuracy: 1.0\n",
      "Batch 30016 / 50000\n",
      "Loss: 155.47956253495067, Accuracy: 1.0\n",
      "Batch 30080 / 50000\n",
      "Loss: 155.51332683768123, Accuracy: 1.0\n",
      "Batch 30144 / 50000\n",
      "Loss: 155.57281644176692, Accuracy: 0.984375\n",
      "Batch 30208 / 50000\n",
      "Loss: 155.6716068936512, Accuracy: 0.96875\n",
      "Batch 30272 / 50000\n",
      "Loss: 155.74656310956925, Accuracy: 0.96875\n",
      "Batch 30336 / 50000\n",
      "Loss: 155.8408743692562, Accuracy: 0.96875\n",
      "Batch 30400 / 50000\n",
      "Loss: 155.91225353907794, Accuracy: 0.984375\n",
      "Batch 30464 / 50000\n",
      "Loss: 156.04237147700042, Accuracy: 0.96875\n",
      "Batch 30528 / 50000\n",
      "Loss: 156.09580607991666, Accuracy: 0.984375\n",
      "Batch 30592 / 50000\n",
      "Loss: 156.13995879795402, Accuracy: 0.984375\n",
      "Batch 30656 / 50000\n",
      "Loss: 156.24022082891315, Accuracy: 0.96875\n",
      "Batch 30720 / 50000\n",
      "Loss: 156.34761205967516, Accuracy: 0.953125\n",
      "Batch 30784 / 50000\n",
      "Loss: 156.36254150420427, Accuracy: 1.0\n",
      "Batch 30848 / 50000\n",
      "Loss: 156.48899231106043, Accuracy: 0.96875\n",
      "Batch 30912 / 50000\n",
      "Loss: 156.59716049581766, Accuracy: 0.953125\n",
      "Batch 30976 / 50000\n",
      "Loss: 156.69541196525097, Accuracy: 0.96875\n",
      "Batch 31040 / 50000\n",
      "Loss: 156.74114213883877, Accuracy: 0.984375\n",
      "Batch 31104 / 50000\n",
      "Loss: 156.78446697816253, Accuracy: 0.984375\n",
      "Batch 31168 / 50000\n",
      "Loss: 156.870743047446, Accuracy: 0.96875\n",
      "Batch 31232 / 50000\n",
      "Loss: 156.95942105725408, Accuracy: 0.984375\n",
      "Batch 31296 / 50000\n",
      "Loss: 157.07976575568318, Accuracy: 0.953125\n",
      "Batch 31360 / 50000\n",
      "Loss: 157.11862044408917, Accuracy: 0.984375\n",
      "Batch 31424 / 50000\n",
      "Loss: 157.1719032637775, Accuracy: 0.984375\n",
      "Batch 31488 / 50000\n",
      "Loss: 157.29214791581035, Accuracy: 0.984375\n",
      "Batch 31552 / 50000\n",
      "Loss: 157.37153917178512, Accuracy: 0.96875\n",
      "Batch 31616 / 50000\n",
      "Loss: 157.4642598219216, Accuracy: 0.96875\n",
      "Batch 31680 / 50000\n",
      "Loss: 157.60262743011117, Accuracy: 0.96875\n",
      "Batch 31744 / 50000\n",
      "Loss: 157.65168957412243, Accuracy: 0.96875\n",
      "Batch 31808 / 50000\n",
      "Loss: 157.80020827054977, Accuracy: 0.96875\n",
      "Batch 31872 / 50000\n",
      "Loss: 157.93431754410267, Accuracy: 0.96875\n",
      "Batch 31936 / 50000\n",
      "Loss: 157.97911931201816, Accuracy: 0.984375\n",
      "Batch 32000 / 50000\n",
      "Loss: 158.20186476036906, Accuracy: 0.9375\n",
      "Batch 32064 / 50000\n",
      "Loss: 158.24661840498447, Accuracy: 0.984375\n",
      "Batch 32128 / 50000\n",
      "Loss: 158.33960561454296, Accuracy: 0.96875\n",
      "Batch 32192 / 50000\n",
      "Loss: 158.4900043606758, Accuracy: 0.953125\n",
      "Batch 32256 / 50000\n",
      "Loss: 158.56087835878134, Accuracy: 0.984375\n",
      "Batch 32320 / 50000\n",
      "Loss: 158.65076753497124, Accuracy: 0.96875\n",
      "Batch 32384 / 50000\n",
      "Loss: 158.72728295624256, Accuracy: 0.984375\n",
      "Batch 32448 / 50000\n",
      "Loss: 158.77900088950992, Accuracy: 0.984375\n",
      "Batch 32512 / 50000\n",
      "Loss: 158.84304390475154, Accuracy: 0.984375\n",
      "Batch 32576 / 50000\n",
      "Loss: 158.85180209577084, Accuracy: 1.0\n",
      "Batch 32640 / 50000\n",
      "Loss: 159.05460819602013, Accuracy: 0.9375\n",
      "Batch 32704 / 50000\n",
      "Loss: 159.16762559115887, Accuracy: 0.953125\n",
      "Batch 32768 / 50000\n",
      "Loss: 159.24231111258268, Accuracy: 0.96875\n",
      "Batch 32832 / 50000\n",
      "Loss: 159.3113583996892, Accuracy: 1.0\n",
      "Batch 32896 / 50000\n",
      "Loss: 159.4693818166852, Accuracy: 0.953125\n",
      "Batch 32960 / 50000\n",
      "Loss: 159.59556787461042, Accuracy: 0.953125\n",
      "Batch 33024 / 50000\n",
      "Loss: 159.64331018179655, Accuracy: 0.984375\n",
      "Batch 33088 / 50000\n",
      "Loss: 159.73016434162855, Accuracy: 0.984375\n",
      "Batch 33152 / 50000\n",
      "Loss: 159.76506593823433, Accuracy: 0.984375\n",
      "Batch 33216 / 50000\n",
      "Loss: 159.8280495479703, Accuracy: 0.984375\n",
      "Batch 33280 / 50000\n",
      "Loss: 159.89454915374517, Accuracy: 0.984375\n",
      "Batch 33344 / 50000\n",
      "Loss: 160.00118071585894, Accuracy: 0.953125\n",
      "Batch 33408 / 50000\n",
      "Loss: 160.16050339490175, Accuracy: 0.953125\n",
      "Batch 33472 / 50000\n",
      "Loss: 160.29612977057695, Accuracy: 0.96875\n",
      "Batch 33536 / 50000\n",
      "Loss: 160.33253510296345, Accuracy: 0.96875\n",
      "Batch 33600 / 50000\n",
      "Loss: 160.45698927342892, Accuracy: 0.953125\n",
      "Batch 33664 / 50000\n",
      "Loss: 160.5017293393612, Accuracy: 0.984375\n",
      "Batch 33728 / 50000\n",
      "Loss: 160.55702050030231, Accuracy: 0.984375\n",
      "Batch 33792 / 50000\n",
      "Loss: 160.79850181937218, Accuracy: 0.90625\n",
      "Batch 33856 / 50000\n",
      "Loss: 160.86810633540154, Accuracy: 0.96875\n",
      "Batch 33920 / 50000\n",
      "Loss: 160.96961897611618, Accuracy: 0.96875\n",
      "Batch 33984 / 50000\n",
      "Loss: 161.09766010940075, Accuracy: 0.953125\n",
      "Batch 34048 / 50000\n",
      "Loss: 161.23510192334652, Accuracy: 0.953125\n",
      "Batch 34112 / 50000\n",
      "Loss: 161.4326254427433, Accuracy: 0.953125\n",
      "Batch 34176 / 50000\n",
      "Loss: 161.46221784688532, Accuracy: 1.0\n",
      "Batch 34240 / 50000\n",
      "Loss: 161.53885267861187, Accuracy: 0.96875\n",
      "Batch 34304 / 50000\n",
      "Loss: 161.57601202838123, Accuracy: 0.984375\n",
      "Batch 34368 / 50000\n",
      "Loss: 161.61900741793215, Accuracy: 0.96875\n",
      "Batch 34432 / 50000\n",
      "Loss: 161.6894096005708, Accuracy: 0.96875\n",
      "Batch 34496 / 50000\n",
      "Loss: 161.7358522322029, Accuracy: 0.984375\n",
      "Batch 34560 / 50000\n",
      "Loss: 161.87485251389444, Accuracy: 0.96875\n",
      "Batch 34624 / 50000\n",
      "Loss: 162.00819294713438, Accuracy: 0.953125\n",
      "Batch 34688 / 50000\n",
      "Loss: 162.02692275308073, Accuracy: 1.0\n",
      "Batch 34752 / 50000\n",
      "Loss: 162.1176063697785, Accuracy: 0.984375\n",
      "Batch 34816 / 50000\n",
      "Loss: 162.2469661962241, Accuracy: 0.96875\n",
      "Batch 34880 / 50000\n",
      "Loss: 162.2728030141443, Accuracy: 1.0\n",
      "Batch 34944 / 50000\n",
      "Loss: 162.30544901825488, Accuracy: 1.0\n",
      "Batch 35008 / 50000\n",
      "Loss: 162.35620594210923, Accuracy: 0.96875\n",
      "Batch 35072 / 50000\n",
      "Loss: 162.41737353987992, Accuracy: 0.984375\n",
      "Batch 35136 / 50000\n",
      "Loss: 162.59329997189343, Accuracy: 0.953125\n",
      "Batch 35200 / 50000\n",
      "Loss: 162.70841106958687, Accuracy: 0.9375\n",
      "Batch 35264 / 50000\n",
      "Loss: 162.745755167678, Accuracy: 0.984375\n",
      "Batch 35328 / 50000\n",
      "Loss: 162.82104018516839, Accuracy: 0.96875\n",
      "Batch 35392 / 50000\n",
      "Loss: 163.0232058931142, Accuracy: 0.953125\n",
      "Batch 35456 / 50000\n",
      "Loss: 163.07050041295588, Accuracy: 1.0\n",
      "Batch 35520 / 50000\n",
      "Loss: 163.16023646481335, Accuracy: 0.96875\n",
      "Batch 35584 / 50000\n",
      "Loss: 163.19460249133408, Accuracy: 1.0\n",
      "Batch 35648 / 50000\n",
      "Loss: 163.3263285625726, Accuracy: 0.953125\n",
      "Batch 35712 / 50000\n",
      "Loss: 163.43034988082945, Accuracy: 0.984375\n",
      "Batch 35776 / 50000\n",
      "Loss: 163.47756340913475, Accuracy: 0.984375\n",
      "Batch 35840 / 50000\n",
      "Loss: 163.5488455053419, Accuracy: 0.953125\n",
      "Batch 35904 / 50000\n",
      "Loss: 163.5622258335352, Accuracy: 1.0\n",
      "Batch 35968 / 50000\n",
      "Loss: 163.63966964930296, Accuracy: 0.984375\n",
      "Batch 36032 / 50000\n",
      "Loss: 163.6958542279899, Accuracy: 0.984375\n",
      "Batch 36096 / 50000\n",
      "Loss: 163.84787818416953, Accuracy: 0.953125\n",
      "Batch 36160 / 50000\n",
      "Loss: 163.90123768895864, Accuracy: 0.96875\n",
      "Batch 36224 / 50000\n",
      "Loss: 163.96829292178154, Accuracy: 0.953125\n",
      "Batch 36288 / 50000\n",
      "Loss: 164.05759711563587, Accuracy: 0.96875\n",
      "Batch 36352 / 50000\n",
      "Loss: 164.0676343832165, Accuracy: 1.0\n",
      "Batch 36416 / 50000\n",
      "Loss: 164.1441728118807, Accuracy: 0.984375\n",
      "Batch 36480 / 50000\n",
      "Loss: 164.21343769319355, Accuracy: 0.96875\n",
      "Batch 36544 / 50000\n",
      "Loss: 164.39137819595635, Accuracy: 0.96875\n",
      "Batch 36608 / 50000\n",
      "Loss: 164.6628412026912, Accuracy: 0.953125\n",
      "Batch 36672 / 50000\n",
      "Loss: 164.7019070070237, Accuracy: 0.984375\n",
      "Batch 36736 / 50000\n",
      "Loss: 164.8116108905524, Accuracy: 0.96875\n",
      "Batch 36800 / 50000\n",
      "Loss: 164.97408626787364, Accuracy: 0.9375\n",
      "Batch 36864 / 50000\n",
      "Loss: 165.0609105657786, Accuracy: 0.984375\n",
      "Batch 36928 / 50000\n",
      "Loss: 165.16395486705005, Accuracy: 0.984375\n",
      "Batch 36992 / 50000\n",
      "Loss: 165.24255103431642, Accuracy: 0.984375\n",
      "Batch 37056 / 50000\n",
      "Loss: 165.29829994402826, Accuracy: 0.984375\n",
      "Batch 37120 / 50000\n",
      "Loss: 165.42558934353292, Accuracy: 0.953125\n",
      "Batch 37184 / 50000\n",
      "Loss: 165.54554344527423, Accuracy: 0.953125\n",
      "Batch 37248 / 50000\n",
      "Loss: 165.61134072206914, Accuracy: 0.96875\n",
      "Batch 37312 / 50000\n",
      "Loss: 165.82036127708852, Accuracy: 0.9375\n",
      "Batch 37376 / 50000\n",
      "Loss: 165.98952970467508, Accuracy: 0.953125\n",
      "Batch 37440 / 50000\n",
      "Loss: 166.04046560265124, Accuracy: 0.984375\n",
      "Batch 37504 / 50000\n",
      "Loss: 166.12306488491595, Accuracy: 0.984375\n",
      "Batch 37568 / 50000\n",
      "Loss: 166.22968763299286, Accuracy: 0.96875\n",
      "Batch 37632 / 50000\n",
      "Loss: 166.26727831922472, Accuracy: 0.984375\n",
      "Batch 37696 / 50000\n",
      "Loss: 166.31507849507034, Accuracy: 0.984375\n",
      "Batch 37760 / 50000\n",
      "Loss: 166.4652298670262, Accuracy: 0.953125\n",
      "Batch 37824 / 50000\n",
      "Loss: 166.51541553623974, Accuracy: 0.984375\n",
      "Batch 37888 / 50000\n",
      "Loss: 166.73646298237145, Accuracy: 0.921875\n",
      "Batch 37952 / 50000\n",
      "Loss: 166.78299589641392, Accuracy: 0.984375\n",
      "Batch 38016 / 50000\n",
      "Loss: 166.8340017106384, Accuracy: 0.984375\n",
      "Batch 38080 / 50000\n",
      "Loss: 166.90062160603702, Accuracy: 0.96875\n",
      "Batch 38144 / 50000\n",
      "Loss: 166.97083908878267, Accuracy: 0.96875\n",
      "Batch 38208 / 50000\n",
      "Loss: 167.12813036702573, Accuracy: 0.96875\n",
      "Batch 38272 / 50000\n",
      "Loss: 167.26406103931367, Accuracy: 0.953125\n",
      "Batch 38336 / 50000\n",
      "Loss: 167.36170857213438, Accuracy: 0.96875\n",
      "Batch 38400 / 50000\n",
      "Loss: 167.41548310406506, Accuracy: 0.984375\n",
      "Batch 38464 / 50000\n",
      "Loss: 167.4996526185423, Accuracy: 0.984375\n",
      "Batch 38528 / 50000\n",
      "Loss: 167.6272337231785, Accuracy: 0.96875\n",
      "Batch 38592 / 50000\n",
      "Loss: 167.72926851548254, Accuracy: 0.96875\n",
      "Batch 38656 / 50000\n",
      "Loss: 167.85691008903086, Accuracy: 0.96875\n",
      "Batch 38720 / 50000\n",
      "Loss: 168.00141186453402, Accuracy: 0.953125\n",
      "Batch 38784 / 50000\n",
      "Loss: 168.02473148703575, Accuracy: 1.0\n",
      "Batch 38848 / 50000\n",
      "Loss: 168.10519946366549, Accuracy: 0.953125\n",
      "Batch 38912 / 50000\n",
      "Loss: 168.13485044799745, Accuracy: 1.0\n",
      "Batch 38976 / 50000\n",
      "Loss: 168.34620588086545, Accuracy: 0.9375\n",
      "Batch 39040 / 50000\n",
      "Loss: 168.47216229699552, Accuracy: 0.921875\n",
      "Batch 39104 / 50000\n",
      "Loss: 168.6193779166788, Accuracy: 0.9375\n",
      "Batch 39168 / 50000\n",
      "Loss: 168.7262585889548, Accuracy: 0.96875\n",
      "Batch 39232 / 50000\n",
      "Loss: 168.78423356451094, Accuracy: 1.0\n",
      "Batch 39296 / 50000\n",
      "Loss: 168.9480662290007, Accuracy: 0.984375\n",
      "Batch 39360 / 50000\n",
      "Loss: 168.9683365020901, Accuracy: 1.0\n",
      "Batch 39424 / 50000\n",
      "Loss: 169.0553952101618, Accuracy: 0.953125\n",
      "Batch 39488 / 50000\n",
      "Loss: 169.1044725831598, Accuracy: 0.984375\n",
      "Batch 39552 / 50000\n",
      "Loss: 169.19259854964912, Accuracy: 0.953125\n",
      "Batch 39616 / 50000\n",
      "Loss: 169.29880751483142, Accuracy: 0.96875\n",
      "Batch 39680 / 50000\n",
      "Loss: 169.35143390111625, Accuracy: 0.984375\n",
      "Batch 39744 / 50000\n",
      "Loss: 169.44502315483987, Accuracy: 0.984375\n",
      "Batch 39808 / 50000\n",
      "Loss: 169.53560059331357, Accuracy: 0.96875\n",
      "Batch 39872 / 50000\n",
      "Loss: 169.6612257566303, Accuracy: 0.96875\n",
      "Batch 39936 / 50000\n",
      "Loss: 169.73106282018125, Accuracy: 0.96875\n",
      "Batch 40000 / 50000\n",
      "Loss: 169.74842878617346, Accuracy: 1.0\n",
      "Batch 40064 / 50000\n",
      "Loss: 169.82748081721365, Accuracy: 0.96875\n",
      "Batch 40128 / 50000\n",
      "Loss: 170.12427803315222, Accuracy: 0.9375\n",
      "Batch 40192 / 50000\n",
      "Loss: 170.33973046578467, Accuracy: 0.9375\n",
      "Batch 40256 / 50000\n",
      "Loss: 170.38020482845604, Accuracy: 0.984375\n",
      "Batch 40320 / 50000\n",
      "Loss: 170.41143740154803, Accuracy: 0.984375\n",
      "Batch 40384 / 50000\n",
      "Loss: 170.4297372121364, Accuracy: 1.0\n",
      "Batch 40448 / 50000\n",
      "Loss: 170.47770897112787, Accuracy: 0.984375\n",
      "Batch 40512 / 50000\n",
      "Loss: 170.52183279581368, Accuracy: 0.984375\n",
      "Batch 40576 / 50000\n",
      "Loss: 170.60273897089064, Accuracy: 0.984375\n",
      "Batch 40640 / 50000\n",
      "Loss: 170.8063160572201, Accuracy: 0.921875\n",
      "Batch 40704 / 50000\n",
      "Loss: 170.90019285865128, Accuracy: 0.984375\n",
      "Batch 40768 / 50000\n",
      "Loss: 170.98328851349652, Accuracy: 0.953125\n",
      "Batch 40832 / 50000\n",
      "Loss: 171.01527937315404, Accuracy: 0.984375\n",
      "Batch 40896 / 50000\n",
      "Loss: 171.04746334068477, Accuracy: 0.984375\n",
      "Batch 40960 / 50000\n",
      "Loss: 171.12703417800367, Accuracy: 0.953125\n",
      "Batch 41024 / 50000\n",
      "Loss: 171.1950118597597, Accuracy: 0.96875\n",
      "Batch 41088 / 50000\n",
      "Loss: 171.24307682551444, Accuracy: 1.0\n",
      "Batch 41152 / 50000\n",
      "Loss: 171.39609492756426, Accuracy: 0.96875\n",
      "Batch 41216 / 50000\n",
      "Loss: 171.64777822233737, Accuracy: 0.953125\n",
      "Batch 41280 / 50000\n",
      "Loss: 171.6701996512711, Accuracy: 1.0\n",
      "Batch 41344 / 50000\n",
      "Loss: 171.78018898144364, Accuracy: 0.984375\n",
      "Batch 41408 / 50000\n",
      "Loss: 171.90426290407777, Accuracy: 0.96875\n",
      "Batch 41472 / 50000\n",
      "Loss: 172.087214384228, Accuracy: 0.9375\n",
      "Batch 41536 / 50000\n",
      "Loss: 172.19452498480678, Accuracy: 0.96875\n",
      "Batch 41600 / 50000\n",
      "Loss: 172.20543750561774, Accuracy: 1.0\n",
      "Batch 41664 / 50000\n",
      "Loss: 172.26999592222273, Accuracy: 0.984375\n",
      "Batch 41728 / 50000\n",
      "Loss: 172.27588625857607, Accuracy: 1.0\n",
      "Batch 41792 / 50000\n",
      "Loss: 172.3638628111221, Accuracy: 0.9375\n",
      "Batch 41856 / 50000\n",
      "Loss: 172.4021861278452, Accuracy: 0.984375\n",
      "Batch 41920 / 50000\n",
      "Loss: 172.41598844947293, Accuracy: 1.0\n",
      "Batch 41984 / 50000\n",
      "Loss: 172.73278978886083, Accuracy: 0.90625\n",
      "Batch 42048 / 50000\n",
      "Loss: 172.7734989193268, Accuracy: 0.984375\n",
      "Batch 42112 / 50000\n",
      "Loss: 172.79867539228871, Accuracy: 1.0\n",
      "Batch 42176 / 50000\n",
      "Loss: 172.96389928879216, Accuracy: 0.953125\n",
      "Batch 42240 / 50000\n",
      "Loss: 172.97343640821055, Accuracy: 1.0\n",
      "Batch 42304 / 50000\n",
      "Loss: 173.05683282809332, Accuracy: 0.953125\n",
      "Batch 42368 / 50000\n",
      "Loss: 173.20369783835486, Accuracy: 0.953125\n",
      "Batch 42432 / 50000\n",
      "Loss: 173.25974526675418, Accuracy: 0.984375\n",
      "Batch 42496 / 50000\n",
      "Loss: 173.35445814253762, Accuracy: 0.984375\n",
      "Batch 42560 / 50000\n",
      "Loss: 173.38330060290173, Accuracy: 0.984375\n",
      "Batch 42624 / 50000\n",
      "Loss: 173.4461518903263, Accuracy: 0.984375\n",
      "Batch 42688 / 50000\n",
      "Loss: 173.5531614725478, Accuracy: 0.96875\n",
      "Batch 42752 / 50000\n",
      "Loss: 173.5565450550057, Accuracy: 1.0\n",
      "Batch 42816 / 50000\n",
      "Loss: 173.60336755076423, Accuracy: 0.96875\n",
      "Batch 42880 / 50000\n",
      "Loss: 173.6844930932857, Accuracy: 0.96875\n",
      "Batch 42944 / 50000\n",
      "Loss: 173.74284811178222, Accuracy: 0.984375\n",
      "Batch 43008 / 50000\n",
      "Loss: 173.83604362467304, Accuracy: 0.96875\n",
      "Batch 43072 / 50000\n",
      "Loss: 173.86967170098796, Accuracy: 1.0\n",
      "Batch 43136 / 50000\n",
      "Loss: 173.964124082122, Accuracy: 0.953125\n",
      "Batch 43200 / 50000\n",
      "Loss: 174.05125503102317, Accuracy: 0.984375\n",
      "Batch 43264 / 50000\n",
      "Loss: 174.0976832159795, Accuracy: 0.96875\n",
      "Batch 43328 / 50000\n",
      "Loss: 174.1124762329273, Accuracy: 1.0\n",
      "Batch 43392 / 50000\n",
      "Loss: 174.20923890499398, Accuracy: 0.96875\n",
      "Batch 43456 / 50000\n",
      "Loss: 174.25770268170163, Accuracy: 0.96875\n",
      "Batch 43520 / 50000\n",
      "Loss: 174.2903870115988, Accuracy: 0.984375\n",
      "Batch 43584 / 50000\n",
      "Loss: 174.38440735591576, Accuracy: 0.9375\n",
      "Batch 43648 / 50000\n",
      "Loss: 174.4414389762096, Accuracy: 0.96875\n",
      "Batch 43712 / 50000\n",
      "Loss: 174.60686895577237, Accuracy: 0.96875\n",
      "Batch 43776 / 50000\n",
      "Loss: 174.67657403973863, Accuracy: 0.984375\n",
      "Batch 43840 / 50000\n",
      "Loss: 174.7026055767201, Accuracy: 0.984375\n",
      "Batch 43904 / 50000\n",
      "Loss: 174.7692602588795, Accuracy: 0.96875\n",
      "Batch 43968 / 50000\n",
      "Loss: 174.7810020581819, Accuracy: 1.0\n",
      "Batch 44032 / 50000\n",
      "Loss: 174.83526653284207, Accuracy: 0.96875\n",
      "Batch 44096 / 50000\n",
      "Loss: 174.9591529467143, Accuracy: 0.984375\n",
      "Batch 44160 / 50000\n",
      "Loss: 175.07109690690413, Accuracy: 0.953125\n",
      "Batch 44224 / 50000\n",
      "Loss: 175.11944338167086, Accuracy: 0.96875\n",
      "Batch 44288 / 50000\n",
      "Loss: 175.17209410155192, Accuracy: 0.96875\n",
      "Batch 44352 / 50000\n",
      "Loss: 175.19626405322924, Accuracy: 1.0\n",
      "Batch 44416 / 50000\n",
      "Loss: 175.2060541450046, Accuracy: 1.0\n",
      "Batch 44480 / 50000\n",
      "Loss: 175.2454200698994, Accuracy: 0.984375\n",
      "Batch 44544 / 50000\n",
      "Loss: 175.38712109765038, Accuracy: 0.96875\n",
      "Batch 44608 / 50000\n",
      "Loss: 175.45512752374634, Accuracy: 0.984375\n",
      "Batch 44672 / 50000\n",
      "Loss: 175.49005427351221, Accuracy: 0.984375\n",
      "Batch 44736 / 50000\n",
      "Loss: 175.57274995883927, Accuracy: 0.984375\n",
      "Batch 44800 / 50000\n",
      "Loss: 175.58852732321247, Accuracy: 1.0\n",
      "Batch 44864 / 50000\n",
      "Loss: 175.6696623279713, Accuracy: 0.96875\n",
      "Batch 44928 / 50000\n",
      "Loss: 176.0091394498013, Accuracy: 0.921875\n",
      "Batch 44992 / 50000\n",
      "Loss: 176.1635291739367, Accuracy: 0.96875\n",
      "Batch 45056 / 50000\n",
      "Loss: 176.18744131876156, Accuracy: 0.984375\n",
      "Batch 45120 / 50000\n",
      "Loss: 176.20072189485654, Accuracy: 1.0\n",
      "Batch 45184 / 50000\n",
      "Loss: 176.28315590089187, Accuracy: 0.96875\n",
      "Batch 45248 / 50000\n",
      "Loss: 176.37098433589563, Accuracy: 0.96875\n",
      "Batch 45312 / 50000\n",
      "Loss: 176.45259108906612, Accuracy: 0.953125\n",
      "Batch 45376 / 50000\n",
      "Loss: 176.5418927348219, Accuracy: 0.984375\n",
      "Batch 45440 / 50000\n",
      "Loss: 176.56972307292745, Accuracy: 1.0\n",
      "Batch 45504 / 50000\n",
      "Loss: 176.6922195139341, Accuracy: 0.96875\n",
      "Batch 45568 / 50000\n",
      "Loss: 176.72340941010043, Accuracy: 1.0\n",
      "Batch 45632 / 50000\n",
      "Loss: 176.777540135663, Accuracy: 0.984375\n",
      "Batch 45696 / 50000\n",
      "Loss: 176.8440819340758, Accuracy: 0.984375\n",
      "Batch 45760 / 50000\n",
      "Loss: 176.90898012695834, Accuracy: 1.0\n",
      "Batch 45824 / 50000\n",
      "Loss: 177.0300819161348, Accuracy: 0.984375\n",
      "Batch 45888 / 50000\n",
      "Loss: 177.05855038342997, Accuracy: 0.984375\n",
      "Batch 45952 / 50000\n",
      "Loss: 177.07710795802996, Accuracy: 1.0\n",
      "Batch 46016 / 50000\n",
      "Loss: 177.16645636362955, Accuracy: 0.96875\n",
      "Batch 46080 / 50000\n",
      "Loss: 177.2155198971741, Accuracy: 0.984375\n",
      "Batch 46144 / 50000\n",
      "Loss: 177.2743063303642, Accuracy: 0.984375\n",
      "Batch 46208 / 50000\n",
      "Loss: 177.28048777161166, Accuracy: 1.0\n",
      "Batch 46272 / 50000\n",
      "Loss: 177.32872468559071, Accuracy: 0.96875\n",
      "Batch 46336 / 50000\n",
      "Loss: 177.4234120431356, Accuracy: 0.984375\n",
      "Batch 46400 / 50000\n",
      "Loss: 177.43006491754204, Accuracy: 1.0\n",
      "Batch 46464 / 50000\n",
      "Loss: 177.46474560443312, Accuracy: 1.0\n",
      "Batch 46528 / 50000\n",
      "Loss: 177.5276517579332, Accuracy: 0.984375\n",
      "Batch 46592 / 50000\n",
      "Loss: 177.64250885043293, Accuracy: 0.96875\n",
      "Batch 46656 / 50000\n",
      "Loss: 177.71112299803644, Accuracy: 0.96875\n",
      "Batch 46720 / 50000\n",
      "Loss: 177.80691013578326, Accuracy: 0.96875\n",
      "Batch 46784 / 50000\n",
      "Loss: 177.94724323693663, Accuracy: 0.953125\n",
      "Batch 46848 / 50000\n",
      "Loss: 178.0611091637984, Accuracy: 0.953125\n",
      "Batch 46912 / 50000\n",
      "Loss: 178.10695705655962, Accuracy: 0.984375\n",
      "Batch 46976 / 50000\n",
      "Loss: 178.13722348120064, Accuracy: 0.984375\n",
      "Batch 47040 / 50000\n",
      "Loss: 178.22936214413494, Accuracy: 0.953125\n",
      "Batch 47104 / 50000\n",
      "Loss: 178.26569853257388, Accuracy: 0.984375\n",
      "Batch 47168 / 50000\n",
      "Loss: 178.2747352849692, Accuracy: 1.0\n",
      "Batch 47232 / 50000\n",
      "Loss: 178.46448667161167, Accuracy: 0.953125\n",
      "Batch 47296 / 50000\n",
      "Loss: 178.4872453827411, Accuracy: 1.0\n",
      "Batch 47360 / 50000\n",
      "Loss: 178.55403267033398, Accuracy: 0.984375\n",
      "Batch 47424 / 50000\n",
      "Loss: 178.73105643875897, Accuracy: 0.921875\n",
      "Batch 47488 / 50000\n",
      "Loss: 178.74062039423734, Accuracy: 1.0\n",
      "Batch 47552 / 50000\n",
      "Loss: 178.75418887007982, Accuracy: 1.0\n",
      "Batch 47616 / 50000\n",
      "Loss: 178.86909679789096, Accuracy: 0.984375\n",
      "Batch 47680 / 50000\n",
      "Loss: 178.93014098796993, Accuracy: 0.984375\n",
      "Batch 47744 / 50000\n",
      "Loss: 178.95351607631892, Accuracy: 1.0\n",
      "Batch 47808 / 50000\n",
      "Loss: 179.01717106532305, Accuracy: 0.96875\n",
      "Batch 47872 / 50000\n",
      "Loss: 179.05572832282633, Accuracy: 0.984375\n",
      "Batch 47936 / 50000\n",
      "Loss: 179.07733295019716, Accuracy: 1.0\n",
      "Batch 48000 / 50000\n",
      "Loss: 179.26017800625414, Accuracy: 0.96875\n",
      "Batch 48064 / 50000\n",
      "Loss: 179.30214172136039, Accuracy: 0.96875\n",
      "Batch 48128 / 50000\n",
      "Loss: 179.3470952352509, Accuracy: 0.984375\n",
      "Batch 48192 / 50000\n",
      "Loss: 179.4002643218264, Accuracy: 0.984375\n",
      "Batch 48256 / 50000\n",
      "Loss: 179.4115243339911, Accuracy: 1.0\n",
      "Batch 48320 / 50000\n",
      "Loss: 179.5942009324208, Accuracy: 0.90625\n",
      "Batch 48384 / 50000\n",
      "Loss: 179.68283952120692, Accuracy: 0.96875\n",
      "Batch 48448 / 50000\n",
      "Loss: 179.7017891695723, Accuracy: 1.0\n",
      "Batch 48512 / 50000\n",
      "Loss: 179.79564455803484, Accuracy: 0.984375\n",
      "Batch 48576 / 50000\n",
      "Loss: 179.9764559334144, Accuracy: 0.96875\n",
      "Batch 48640 / 50000\n",
      "Loss: 180.0052381036803, Accuracy: 1.0\n",
      "Batch 48704 / 50000\n",
      "Loss: 180.06593672465533, Accuracy: 0.953125\n",
      "Batch 48768 / 50000\n",
      "Loss: 180.1783640263602, Accuracy: 0.984375\n",
      "Batch 48832 / 50000\n",
      "Loss: 180.2252277834341, Accuracy: 0.984375\n",
      "Batch 48896 / 50000\n",
      "Loss: 180.24214452784508, Accuracy: 1.0\n",
      "Batch 48960 / 50000\n",
      "Loss: 180.2678321665153, Accuracy: 1.0\n",
      "Batch 49024 / 50000\n",
      "Loss: 180.35638721939176, Accuracy: 0.984375\n",
      "Batch 49088 / 50000\n",
      "Loss: 180.4010081132874, Accuracy: 0.984375\n",
      "Batch 49152 / 50000\n",
      "Loss: 180.57118059601635, Accuracy: 0.953125\n",
      "Batch 49216 / 50000\n",
      "Loss: 180.70394766237587, Accuracy: 0.96875\n",
      "Batch 49280 / 50000\n",
      "Loss: 180.7630224218592, Accuracy: 0.984375\n",
      "Batch 49344 / 50000\n",
      "Loss: 180.7902393369004, Accuracy: 1.0\n",
      "Batch 49408 / 50000\n",
      "Loss: 180.8295222474262, Accuracy: 0.984375\n",
      "Batch 49472 / 50000\n",
      "Loss: 181.11288368236274, Accuracy: 0.9375\n",
      "Batch 49536 / 50000\n",
      "Loss: 181.14096920844167, Accuracy: 1.0\n",
      "Batch 49600 / 50000\n",
      "Loss: 181.1836935384199, Accuracy: 0.984375\n",
      "Batch 49664 / 50000\n",
      "Loss: 181.23564873915166, Accuracy: 0.96875\n",
      "Batch 49728 / 50000\n",
      "Loss: 181.27787058334798, Accuracy: 0.96875\n",
      "Batch 49792 / 50000\n",
      "Loss: 181.31432178150862, Accuracy: 0.984375\n",
      "Batch 49856 / 50000\n",
      "Loss: 181.34100320469588, Accuracy: 1.0\n",
      "Batch 49920 / 50000\n",
      "Loss: 181.38490279670805, Accuracy: 0.984375\n",
      "Batch 49984 / 50000\n",
      "Loss: 181.469577296637, Accuracy: 0.96875\n",
      "Batch 50048 / 50000\n",
      "Loss: 181.55021728482097, Accuracy: 0.9375\n",
      "Val Loss: 0.10896840691566467, Val Accuracy: 0.953125\n",
      "Val Loss: 0.17565561830997467, Val Accuracy: 0.96875\n",
      "Val Loss: 0.21514945477247238, Val Accuracy: 0.984375\n",
      "Val Loss: 0.22457960713654757, Val Accuracy: 1.0\n",
      "Val Loss: 0.27215432096272707, Val Accuracy: 0.984375\n",
      "Val Loss: 0.36872888822108507, Val Accuracy: 0.96875\n",
      "Val Loss: 0.4036169843748212, Val Accuracy: 0.984375\n",
      "Val Loss: 0.4384123282507062, Val Accuracy: 0.984375\n",
      "Val Loss: 0.48219933826476336, Val Accuracy: 0.984375\n",
      "Val Loss: 0.5245435619726777, Val Accuracy: 0.984375\n",
      "Val Loss: 0.5900044376030564, Val Accuracy: 0.984375\n",
      "Val Loss: 0.7384650940075517, Val Accuracy: 0.921875\n",
      "Val Loss: 0.8513234192505479, Val Accuracy: 0.96875\n",
      "Val Loss: 0.9184426674619317, Val Accuracy: 0.96875\n",
      "Val Loss: 1.0809919694438577, Val Accuracy: 0.953125\n",
      "Val Loss: 1.1501962328329682, Val Accuracy: 0.96875\n",
      "Val Loss: 1.2246288815513253, Val Accuracy: 0.96875\n",
      "Val Loss: 1.239008016884327, Val Accuracy: 1.0\n",
      "Val Loss: 1.3179774209856987, Val Accuracy: 0.96875\n",
      "Val Loss: 1.4450776800513268, Val Accuracy: 0.953125\n",
      "Val Loss: 1.461177997291088, Val Accuracy: 1.0\n",
      "Val Loss: 1.520615678280592, Val Accuracy: 0.984375\n",
      "Val Loss: 1.5319883292540908, Val Accuracy: 1.0\n",
      "Val Loss: 1.5657450100407004, Val Accuracy: 1.0\n",
      "Val Loss: 1.7865865966305137, Val Accuracy: 0.953125\n",
      "Val Loss: 1.7971419990062714, Val Accuracy: 1.0\n",
      "Val Loss: 1.8420479968190193, Val Accuracy: 0.984375\n",
      "Val Loss: 1.8689859323203564, Val Accuracy: 1.0\n",
      "Val Loss: 2.070721860975027, Val Accuracy: 0.9375\n",
      "Val Loss: 2.241218324750662, Val Accuracy: 0.9375\n",
      "Val Loss: 2.271494474261999, Val Accuracy: 0.984375\n",
      "Val Loss: 2.290849879384041, Val Accuracy: 1.0\n",
      "Val Loss: 2.3471493907272816, Val Accuracy: 0.984375\n",
      "Val Loss: 2.373667409643531, Val Accuracy: 1.0\n",
      "Val Loss: 2.429018249735236, Val Accuracy: 0.984375\n",
      "Val Loss: 2.4592811167240143, Val Accuracy: 0.984375\n",
      "Val Loss: 2.4792810548096895, Val Accuracy: 1.0\n",
      "Val Loss: 2.5100249256938696, Val Accuracy: 1.0\n",
      "Val Loss: 2.526831431314349, Val Accuracy: 0.984375\n",
      "Val Loss: 2.617158142849803, Val Accuracy: 0.984375\n",
      "Val Loss: 2.767362965270877, Val Accuracy: 0.96875\n",
      "Val Loss: 2.797722624614835, Val Accuracy: 1.0\n",
      "Val Loss: 2.8337792921811342, Val Accuracy: 0.984375\n",
      "Val Loss: 2.8653396386653185, Val Accuracy: 1.0\n",
      "Val Loss: 2.918682051822543, Val Accuracy: 0.96875\n",
      "Val Loss: 2.962444359436631, Val Accuracy: 0.984375\n",
      "Val Loss: 3.0268582571297884, Val Accuracy: 0.984375\n",
      "Val Loss: 3.054461432620883, Val Accuracy: 1.0\n",
      "Val Loss: 3.0658942768350244, Val Accuracy: 1.0\n",
      "Val Loss: 3.1329425955191255, Val Accuracy: 0.96875\n",
      "Val Loss: 3.2212701747193933, Val Accuracy: 0.984375\n",
      "Val Loss: 3.3754534730687737, Val Accuracy: 0.9375\n",
      "Val Loss: 3.525178358890116, Val Accuracy: 0.984375\n",
      "Val Loss: 3.6058229887858033, Val Accuracy: 0.96875\n",
      "Val Loss: 3.642013960517943, Val Accuracy: 0.984375\n",
      "Val Loss: 3.7558961072936654, Val Accuracy: 0.984375\n",
      "Val Loss: 3.82698589656502, Val Accuracy: 0.984375\n",
      "Val Loss: 3.930980139411986, Val Accuracy: 0.984375\n",
      "Val Loss: 3.973992240615189, Val Accuracy: 0.984375\n",
      "Val Loss: 4.008074150420725, Val Accuracy: 1.0\n",
      "Val Loss: 4.1321114981547, Val Accuracy: 0.953125\n",
      "Val Loss: 4.298755251802504, Val Accuracy: 0.96875\n",
      "Val Loss: 4.374343910254538, Val Accuracy: 0.984375\n",
      "Val Loss: 4.437863164581358, Val Accuracy: 0.96875\n",
      "Val Loss: 4.455480446107686, Val Accuracy: 1.0\n",
      "Val Loss: 4.460278597660363, Val Accuracy: 1.0\n",
      "Val Loss: 4.488175469450653, Val Accuracy: 0.984375\n",
      "Val Loss: 4.518462269566953, Val Accuracy: 0.984375\n",
      "Val Loss: 4.575159131549299, Val Accuracy: 0.984375\n",
      "Val Loss: 4.582084310241044, Val Accuracy: 1.0\n",
      "Val Loss: 4.666021128185093, Val Accuracy: 0.953125\n",
      "Val Loss: 4.725441732443869, Val Accuracy: 0.96875\n",
      "Val Loss: 4.7943999553099275, Val Accuracy: 0.96875\n",
      "Val Loss: 4.827192877419293, Val Accuracy: 0.984375\n",
      "Val Loss: 4.9060454638674855, Val Accuracy: 0.96875\n",
      "Val Loss: 4.928493634797633, Val Accuracy: 0.984375\n",
      "Val Loss: 4.998199381865561, Val Accuracy: 0.96875\n",
      "Val Loss: 5.111255646683276, Val Accuracy: 0.96875\n",
      "Val Loss: 5.131365993060172, Val Accuracy: 1.0\n",
      "Val Loss: 5.149216917343438, Val Accuracy: 1.0\n",
      "Val Loss: 5.262038220651448, Val Accuracy: 0.953125\n",
      "Val Loss: 5.296773766167462, Val Accuracy: 0.984375\n",
      "Val Loss: 5.3073338540270925, Val Accuracy: 1.0\n",
      "Val Loss: 5.3464896166697145, Val Accuracy: 0.984375\n",
      "Val Loss: 5.419777818955481, Val Accuracy: 0.96875\n",
      "Val Loss: 5.606557571329176, Val Accuracy: 0.96875\n",
      "Val Loss: 5.609472692711279, Val Accuracy: 1.0\n",
      "Val Loss: 5.623974244808778, Val Accuracy: 1.0\n",
      "Val Loss: 5.701574098085985, Val Accuracy: 0.96875\n",
      "Val Loss: 5.793357480084524, Val Accuracy: 0.953125\n",
      "Val Loss: 5.825775541132316, Val Accuracy: 0.984375\n",
      "Val Loss: 5.86431298754178, Val Accuracy: 0.984375\n",
      "Val Loss: 6.000410955166444, Val Accuracy: 0.984375\n",
      "Val Loss: 6.053062103455886, Val Accuracy: 0.96875\n",
      "Val Loss: 6.147785156732425, Val Accuracy: 0.984375\n",
      "Val Loss: 6.2339760509785265, Val Accuracy: 0.96875\n",
      "Val Loss: 6.250244010007009, Val Accuracy: 0.984375\n",
      "Val Loss: 6.29015177465044, Val Accuracy: 0.984375\n",
      "Val Loss: 6.3004499084781855, Val Accuracy: 1.0\n",
      "Val Loss: 6.3083156298380345, Val Accuracy: 1.0\n",
      "Val Loss: 6.355744695989415, Val Accuracy: 0.96875\n",
      "Val Loss: 6.404518130002543, Val Accuracy: 0.984375\n",
      "Val Loss: 6.518868411658332, Val Accuracy: 0.96875\n",
      "Val Loss: 6.576322081265971, Val Accuracy: 0.984375\n",
      "Val Loss: 6.679423111258075, Val Accuracy: 0.96875\n",
      "Val Loss: 6.758284943876788, Val Accuracy: 0.96875\n",
      "Val Loss: 6.805525622097775, Val Accuracy: 0.984375\n",
      "Val Loss: 6.819264436839148, Val Accuracy: 1.0\n",
      "Val Loss: 6.826627026544884, Val Accuracy: 1.0\n",
      "Val Loss: 6.889192621456459, Val Accuracy: 0.984375\n",
      "Val Loss: 6.995464536594227, Val Accuracy: 0.984375\n",
      "Val Loss: 7.009796161437407, Val Accuracy: 1.0\n",
      "Val Loss: 7.068622555816546, Val Accuracy: 0.984375\n",
      "Val Loss: 7.186685357475653, Val Accuracy: 0.984375\n",
      "Val Loss: 7.284793611848727, Val Accuracy: 0.953125\n",
      "Val Loss: 7.290166512830183, Val Accuracy: 1.0\n",
      "Val Loss: 7.327795323682949, Val Accuracy: 0.984375\n",
      "Val Loss: 7.386215128703043, Val Accuracy: 0.96875\n",
      "Val Loss: 7.473507196409628, Val Accuracy: 0.96875\n",
      "Val Loss: 7.5247122279834, Val Accuracy: 0.96875\n",
      "Val Loss: 7.5685389682184905, Val Accuracy: 0.984375\n",
      "Val Loss: 7.587911912007257, Val Accuracy: 1.0\n",
      "Val Loss: 7.601823202101514, Val Accuracy: 1.0\n",
      "Val Loss: 7.6824410546105355, Val Accuracy: 0.984375\n",
      "Val Loss: 7.739324676571414, Val Accuracy: 0.984375\n",
      "Val Loss: 7.7657412372063845, Val Accuracy: 0.984375\n",
      "Val Loss: 7.803751342697069, Val Accuracy: 0.984375\n",
      "Val Loss: 7.863197778118774, Val Accuracy: 0.984375\n",
      "Val Loss: 7.864913368830457, Val Accuracy: 1.0\n",
      "Val Loss: 7.960778201231733, Val Accuracy: 0.9375\n",
      "Val Loss: 7.989675434539095, Val Accuracy: 1.0\n",
      "Val Loss: 8.140980305382982, Val Accuracy: 0.984375\n",
      "Val Loss: 8.178229162702337, Val Accuracy: 0.984375\n",
      "Val Loss: 8.203039346495643, Val Accuracy: 1.0\n",
      "Val Loss: 8.270198865095153, Val Accuracy: 0.96875\n",
      "Val Loss: 8.310001319507137, Val Accuracy: 0.984375\n",
      "Val Loss: 8.343296559760347, Val Accuracy: 1.0\n",
      "Val Loss: 8.36880809837021, Val Accuracy: 1.0\n",
      "Val Loss: 8.411315702134743, Val Accuracy: 0.984375\n",
      "Val Loss: 8.576490260893479, Val Accuracy: 0.96875\n",
      "Val Loss: 8.877819873625413, Val Accuracy: 0.96875\n",
      "Val Loss: 8.903235741192475, Val Accuracy: 0.984375\n",
      "Val Loss: 8.93500828021206, Val Accuracy: 1.0\n",
      "Val Loss: 9.067877359921113, Val Accuracy: 0.96875\n",
      "Val Loss: 9.131039120489731, Val Accuracy: 0.984375\n",
      "Val Loss: 9.24551959359087, Val Accuracy: 0.96875\n",
      "Val Loss: 9.34586487733759, Val Accuracy: 0.96875\n",
      "Val Loss: 9.413238845998421, Val Accuracy: 0.96875\n",
      "Val Loss: 9.453828659141436, Val Accuracy: 0.984375\n",
      "Val Loss: 9.543994296574965, Val Accuracy: 0.96875\n",
      "Val Loss: 9.580308914417401, Val Accuracy: 0.984375\n",
      "Val Loss: 9.677397333318368, Val Accuracy: 0.96875\n",
      "Val Loss: 9.704677114496008, Val Accuracy: 1.0\n",
      "Val Loss: 9.802306788275018, Val Accuracy: 0.953125\n",
      "Val Loss: 9.846323253819719, Val Accuracy: 0.984375\n",
      "Val Loss: 9.872529834741727, Val Accuracy: 0.984375\n",
      "Val Loss: 10.197069704765454, Val Accuracy: 0.9375\n",
      "Epoch 1, Loss: 0.2322, Accuracy: 0.9268\n",
      "Val Loss: 0.0649, Val Accuracy: 0.9805\n",
      "Batch 64 / 50000\n",
      "Loss: 0.21583741903305054, Accuracy: 0.953125\n",
      "Batch 128 / 50000\n",
      "Loss: 0.3811114877462387, Accuracy: 0.96875\n",
      "Batch 192 / 50000\n",
      "Loss: 0.43183259665966034, Accuracy: 0.984375\n",
      "Batch 256 / 50000\n",
      "Loss: 0.5419983267784119, Accuracy: 0.984375\n",
      "Batch 320 / 50000\n",
      "Loss: 0.6160310730338097, Accuracy: 0.984375\n",
      "Batch 384 / 50000\n",
      "Loss: 0.627916157245636, Accuracy: 1.0\n",
      "Batch 448 / 50000\n",
      "Loss: 0.7155245989561081, Accuracy: 0.953125\n",
      "Batch 512 / 50000\n",
      "Loss: 0.7462392579764128, Accuracy: 0.984375\n",
      "Batch 576 / 50000\n",
      "Loss: 0.7771492712199688, Accuracy: 1.0\n",
      "Batch 640 / 50000\n",
      "Loss: 0.9368838183581829, Accuracy: 0.96875\n",
      "Batch 704 / 50000\n",
      "Loss: 0.9761567451059818, Accuracy: 0.984375\n",
      "Batch 768 / 50000\n",
      "Loss: 1.0398186929523945, Accuracy: 0.984375\n",
      "Batch 832 / 50000\n",
      "Loss: 1.0625894851982594, Accuracy: 1.0\n",
      "Batch 896 / 50000\n",
      "Loss: 1.0831274930387735, Accuracy: 0.984375\n",
      "Batch 960 / 50000\n",
      "Loss: 1.1094115413725376, Accuracy: 0.984375\n",
      "Batch 1024 / 50000\n",
      "Loss: 1.2031297944486141, Accuracy: 0.984375\n",
      "Batch 1088 / 50000\n",
      "Loss: 1.2810759358108044, Accuracy: 0.96875\n",
      "Batch 1152 / 50000\n",
      "Loss: 1.3504287488758564, Accuracy: 0.984375\n",
      "Batch 1216 / 50000\n",
      "Loss: 1.4635972864925861, Accuracy: 0.953125\n",
      "Batch 1280 / 50000\n",
      "Loss: 1.5064139775931835, Accuracy: 0.984375\n",
      "Batch 1344 / 50000\n",
      "Loss: 1.5837464444339275, Accuracy: 0.984375\n",
      "Batch 1408 / 50000\n",
      "Loss: 1.6947499625384808, Accuracy: 0.953125\n",
      "Batch 1472 / 50000\n",
      "Loss: 1.7210534494370222, Accuracy: 0.984375\n",
      "Batch 1536 / 50000\n",
      "Loss: 1.7982098814100027, Accuracy: 0.984375\n",
      "Batch 1600 / 50000\n",
      "Loss: 1.8351454082876444, Accuracy: 0.984375\n",
      "Batch 1664 / 50000\n",
      "Loss: 1.8528039027005434, Accuracy: 1.0\n",
      "Batch 1728 / 50000\n",
      "Loss: 1.896175978705287, Accuracy: 0.984375\n",
      "Batch 1792 / 50000\n",
      "Loss: 1.9559901114553213, Accuracy: 0.984375\n",
      "Batch 1856 / 50000\n",
      "Loss: 1.9883609469980001, Accuracy: 1.0\n",
      "Batch 1920 / 50000\n",
      "Loss: 2.0387354660779238, Accuracy: 0.96875\n",
      "Batch 1984 / 50000\n",
      "Loss: 2.046536713372916, Accuracy: 1.0\n",
      "Batch 2048 / 50000\n",
      "Loss: 2.1691758478991687, Accuracy: 0.953125\n",
      "Batch 2112 / 50000\n",
      "Loss: 2.1991513478569686, Accuracy: 0.984375\n",
      "Batch 2176 / 50000\n",
      "Loss: 2.213343139272183, Accuracy: 1.0\n",
      "Batch 2240 / 50000\n",
      "Loss: 2.228519208263606, Accuracy: 1.0\n",
      "Batch 2304 / 50000\n",
      "Loss: 2.3643939117901027, Accuracy: 0.96875\n",
      "Batch 2368 / 50000\n",
      "Loss: 2.392162177246064, Accuracy: 0.984375\n",
      "Batch 2432 / 50000\n",
      "Loss: 2.473372246604413, Accuracy: 0.984375\n",
      "Batch 2496 / 50000\n",
      "Loss: 2.6730260807089508, Accuracy: 0.953125\n",
      "Batch 2560 / 50000\n",
      "Loss: 2.6841412116773427, Accuracy: 1.0\n",
      "Batch 2624 / 50000\n",
      "Loss: 2.74491372006014, Accuracy: 0.984375\n",
      "Batch 2688 / 50000\n",
      "Loss: 2.7492006393149495, Accuracy: 1.0\n",
      "Batch 2752 / 50000\n",
      "Loss: 2.757551479153335, Accuracy: 1.0\n",
      "Batch 2816 / 50000\n",
      "Loss: 2.801107461564243, Accuracy: 0.984375\n",
      "Batch 2880 / 50000\n",
      "Loss: 2.8423127001151443, Accuracy: 0.984375\n",
      "Batch 2944 / 50000\n",
      "Loss: 2.872081737034023, Accuracy: 0.984375\n",
      "Batch 3008 / 50000\n",
      "Loss: 2.9156345007941127, Accuracy: 0.984375\n",
      "Batch 3072 / 50000\n",
      "Loss: 2.938533346168697, Accuracy: 1.0\n",
      "Batch 3136 / 50000\n",
      "Loss: 2.990911561064422, Accuracy: 0.984375\n",
      "Batch 3200 / 50000\n",
      "Loss: 3.110710914246738, Accuracy: 0.953125\n",
      "Batch 3264 / 50000\n",
      "Loss: 3.2541871024295688, Accuracy: 0.96875\n",
      "Batch 3328 / 50000\n",
      "Loss: 3.2605736143887043, Accuracy: 1.0\n",
      "Batch 3392 / 50000\n",
      "Loss: 3.331231329590082, Accuracy: 0.984375\n",
      "Batch 3456 / 50000\n",
      "Loss: 3.375048141926527, Accuracy: 0.984375\n",
      "Batch 3520 / 50000\n",
      "Loss: 3.4407681487500668, Accuracy: 0.984375\n",
      "Batch 3584 / 50000\n",
      "Loss: 3.4608918577432632, Accuracy: 1.0\n",
      "Batch 3648 / 50000\n",
      "Loss: 3.539150319993496, Accuracy: 0.984375\n",
      "Batch 3712 / 50000\n",
      "Loss: 3.580269355326891, Accuracy: 0.984375\n",
      "Batch 3776 / 50000\n",
      "Loss: 3.585075132548809, Accuracy: 1.0\n",
      "Batch 3840 / 50000\n",
      "Loss: 3.624046456068754, Accuracy: 1.0\n",
      "Batch 3904 / 50000\n",
      "Loss: 3.6875345669686794, Accuracy: 0.984375\n",
      "Batch 3968 / 50000\n",
      "Loss: 3.7314179837703705, Accuracy: 0.984375\n",
      "Batch 4032 / 50000\n",
      "Loss: 3.786431163549423, Accuracy: 0.984375\n",
      "Batch 4096 / 50000\n",
      "Loss: 3.78967936267145, Accuracy: 1.0\n",
      "Batch 4160 / 50000\n",
      "Loss: 3.94758754898794, Accuracy: 0.953125\n",
      "Batch 4224 / 50000\n",
      "Loss: 3.9918991022277623, Accuracy: 0.984375\n",
      "Batch 4288 / 50000\n",
      "Loss: 4.061819828348234, Accuracy: 0.953125\n",
      "Batch 4352 / 50000\n",
      "Loss: 4.1203132786322385, Accuracy: 0.96875\n",
      "Batch 4416 / 50000\n",
      "Loss: 4.171772069530562, Accuracy: 0.96875\n",
      "Batch 4480 / 50000\n",
      "Loss: 4.207192893372849, Accuracy: 0.984375\n",
      "Batch 4544 / 50000\n",
      "Loss: 4.23773282696493, Accuracy: 0.984375\n",
      "Batch 4608 / 50000\n",
      "Loss: 4.3187711604405195, Accuracy: 0.984375\n",
      "Batch 4672 / 50000\n",
      "Loss: 4.414073734777048, Accuracy: 0.984375\n",
      "Batch 4736 / 50000\n",
      "Loss: 4.548942088382319, Accuracy: 0.953125\n",
      "Batch 4800 / 50000\n",
      "Loss: 4.555607236223295, Accuracy: 1.0\n",
      "Batch 4864 / 50000\n",
      "Loss: 4.572285606758669, Accuracy: 1.0\n",
      "Batch 4928 / 50000\n",
      "Loss: 4.596767240436748, Accuracy: 1.0\n",
      "Batch 4992 / 50000\n",
      "Loss: 4.615576169686392, Accuracy: 1.0\n",
      "Batch 5056 / 50000\n",
      "Loss: 4.6628784604836255, Accuracy: 0.96875\n",
      "Batch 5120 / 50000\n",
      "Loss: 4.6801479638088495, Accuracy: 1.0\n",
      "Batch 5184 / 50000\n",
      "Loss: 4.701349397422746, Accuracy: 0.984375\n",
      "Batch 5248 / 50000\n",
      "Loss: 4.7437120352406055, Accuracy: 0.984375\n",
      "Batch 5312 / 50000\n",
      "Loss: 4.787282281788066, Accuracy: 0.96875\n",
      "Batch 5376 / 50000\n",
      "Loss: 4.801088338950649, Accuracy: 1.0\n",
      "Batch 5440 / 50000\n",
      "Loss: 4.80737364361994, Accuracy: 1.0\n",
      "Batch 5504 / 50000\n",
      "Loss: 4.897657946450636, Accuracy: 0.984375\n",
      "Batch 5568 / 50000\n",
      "Loss: 4.938071452779695, Accuracy: 0.984375\n",
      "Batch 5632 / 50000\n",
      "Loss: 4.9870624400209635, Accuracy: 0.984375\n",
      "Batch 5696 / 50000\n",
      "Loss: 5.114129111869261, Accuracy: 0.953125\n",
      "Batch 5760 / 50000\n",
      "Loss: 5.156627208692953, Accuracy: 0.96875\n",
      "Batch 5824 / 50000\n",
      "Loss: 5.194688373012468, Accuracy: 0.984375\n",
      "Batch 5888 / 50000\n",
      "Loss: 5.231111169559881, Accuracy: 1.0\n",
      "Batch 5952 / 50000\n",
      "Loss: 5.345052488846704, Accuracy: 0.96875\n",
      "Batch 6016 / 50000\n",
      "Loss: 5.485620581312105, Accuracy: 0.953125\n",
      "Batch 6080 / 50000\n",
      "Loss: 5.56381528894417, Accuracy: 0.96875\n",
      "Batch 6144 / 50000\n",
      "Loss: 5.607882943702862, Accuracy: 0.984375\n",
      "Batch 6208 / 50000\n",
      "Loss: 5.757490721298382, Accuracy: 0.953125\n",
      "Batch 6272 / 50000\n",
      "Loss: 5.871494796825573, Accuracy: 0.96875\n",
      "Batch 6336 / 50000\n",
      "Loss: 5.9131237275432795, Accuracy: 0.984375\n",
      "Batch 6400 / 50000\n",
      "Loss: 5.970083997352049, Accuracy: 0.984375\n",
      "Batch 6464 / 50000\n",
      "Loss: 6.007808346999809, Accuracy: 0.984375\n",
      "Batch 6528 / 50000\n",
      "Loss: 6.027224673656747, Accuracy: 1.0\n",
      "Batch 6592 / 50000\n",
      "Loss: 6.034035339253023, Accuracy: 1.0\n",
      "Batch 6656 / 50000\n",
      "Loss: 6.074285513954237, Accuracy: 0.984375\n",
      "Batch 6720 / 50000\n",
      "Loss: 6.143322079675272, Accuracy: 0.984375\n",
      "Batch 6784 / 50000\n",
      "Loss: 6.177575352834538, Accuracy: 0.984375\n",
      "Batch 6848 / 50000\n",
      "Loss: 6.189134019659832, Accuracy: 1.0\n",
      "Batch 6912 / 50000\n",
      "Loss: 6.3747171938885, Accuracy: 0.9375\n",
      "Batch 6976 / 50000\n",
      "Loss: 6.465044702636078, Accuracy: 0.984375\n",
      "Batch 7040 / 50000\n",
      "Loss: 6.490655952366069, Accuracy: 1.0\n",
      "Batch 7104 / 50000\n",
      "Loss: 6.529396736295894, Accuracy: 0.984375\n",
      "Batch 7168 / 50000\n",
      "Loss: 6.576590792508796, Accuracy: 0.984375\n",
      "Batch 7232 / 50000\n",
      "Loss: 6.66966209677048, Accuracy: 0.953125\n",
      "Batch 7296 / 50000\n",
      "Loss: 6.70907694962807, Accuracy: 0.984375\n",
      "Batch 7360 / 50000\n",
      "Loss: 6.820971415610984, Accuracy: 0.96875\n",
      "Batch 7424 / 50000\n",
      "Loss: 6.961254210444167, Accuracy: 0.953125\n",
      "Batch 7488 / 50000\n",
      "Loss: 7.051709161372855, Accuracy: 0.984375\n",
      "Batch 7552 / 50000\n",
      "Loss: 7.119430595310405, Accuracy: 0.984375\n",
      "Batch 7616 / 50000\n",
      "Loss: 7.1395705689210445, Accuracy: 1.0\n",
      "Batch 7680 / 50000\n",
      "Loss: 7.22843732428737, Accuracy: 0.96875\n",
      "Batch 7744 / 50000\n",
      "Loss: 7.297150013269857, Accuracy: 0.984375\n",
      "Batch 7808 / 50000\n",
      "Loss: 7.353212015004829, Accuracy: 0.984375\n",
      "Batch 7872 / 50000\n",
      "Loss: 7.3646586772520095, Accuracy: 1.0\n",
      "Batch 7936 / 50000\n",
      "Loss: 7.385723273502663, Accuracy: 1.0\n",
      "Batch 8000 / 50000\n",
      "Loss: 7.462899896549061, Accuracy: 0.953125\n",
      "Batch 8064 / 50000\n",
      "Loss: 7.486849540146068, Accuracy: 0.984375\n",
      "Batch 8128 / 50000\n",
      "Loss: 7.611876332433894, Accuracy: 0.96875\n",
      "Batch 8192 / 50000\n",
      "Loss: 7.626850187079981, Accuracy: 1.0\n",
      "Batch 8256 / 50000\n",
      "Loss: 7.683053679065779, Accuracy: 0.984375\n",
      "Batch 8320 / 50000\n",
      "Loss: 7.750084906117991, Accuracy: 0.984375\n",
      "Batch 8384 / 50000\n",
      "Loss: 7.85595954884775, Accuracy: 0.984375\n",
      "Batch 8448 / 50000\n",
      "Loss: 7.898285667644814, Accuracy: 0.96875\n",
      "Batch 8512 / 50000\n",
      "Loss: 7.999708916293457, Accuracy: 0.96875\n",
      "Batch 8576 / 50000\n",
      "Loss: 8.048090501921251, Accuracy: 0.984375\n",
      "Batch 8640 / 50000\n",
      "Loss: 8.151338196592405, Accuracy: 0.96875\n",
      "Batch 8704 / 50000\n",
      "Loss: 8.224119707243517, Accuracy: 0.984375\n",
      "Batch 8768 / 50000\n",
      "Loss: 8.285718842642382, Accuracy: 0.96875\n",
      "Batch 8832 / 50000\n",
      "Loss: 8.348358660237864, Accuracy: 0.96875\n",
      "Batch 8896 / 50000\n",
      "Loss: 8.359841984929517, Accuracy: 1.0\n",
      "Batch 8960 / 50000\n",
      "Loss: 8.42407609266229, Accuracy: 0.96875\n",
      "Batch 9024 / 50000\n",
      "Loss: 8.457287923665717, Accuracy: 0.984375\n",
      "Batch 9088 / 50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y)\n\u001b[0;32m     17\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 19\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torchmetrics\\metric.py:303\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torchmetrics\\metric.py:372\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torchmetrics\\metric.py:465\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torchmetrics\\classification\\stat_scores.py:331\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[1;32m--> 331\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m    335\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[0;32m    336\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[0;32m    337\u001b[0m )\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torchmetrics\\functional\\classification\\stat_scores.py:307\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[1;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and `preds` should be (N, C, ...).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[1;32m--> 307\u001b[0m num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    308\u001b[0m check \u001b[38;5;241m=\u001b[39m num_unique_values \u001b[38;5;241m>\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check:\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torch\\_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torch\\_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torch\\functional.py:976\u001b[0m, in \u001b[0;36m_return_output\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[1;32m--> 976\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mm:\\ML\\ML_regs\\.venv\\Lib\\site-packages\\torch\\functional.py:890\u001b[0m, in \u001b[0;36m_unique_impl\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m    882\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    884\u001b[0m         dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[0;32m    888\u001b[0m     )\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 890\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    train_loss, train_acc = 0, 0\n",
    "    cumulative_batch = 0\n",
    "\n",
    "\n",
    "    for X, y in trainloader:\n",
    "        \n",
    "        model_vgg16.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        cumulative_batch += BATCH_SIZE\n",
    "        print(f'Batch {cumulative_batch} / 50000')\n",
    "\n",
    "        y_pred = model_vgg16(X)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc += acc\n",
    "        print(f'Loss: {train_loss}, Accuracy: {acc}')\n",
    "\n",
    "        optimizer_adam.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_adam.step()\n",
    "    \n",
    "    train_loss /= len(trainloader)\n",
    "    train_acc /= len(trainloader)\n",
    "\n",
    "    val_loss, val_acc = 0, 0\n",
    "    model_vgg16.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in valloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model_vgg16(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            val_loss += loss.item()\n",
    "            acc = accuracy(y_pred, y)\n",
    "            val_acc += acc\n",
    "            print(f'Val Loss: {val_loss}, Val Accuracy: {acc}')\n",
    "        \n",
    "        val_loss /= len(valloader)\n",
    "        val_acc /= len(valloader)\n",
    "\n",
    "\n",
    "    log_writer.add_scalars(main_tag=\"Loss\", tag_scalar_dict={\"train/loss\": train_loss, \"val/loss\": val_loss}, global_step=epoch)\n",
    "    log_writer.add_scalars(main_tag=\"Accuracy\", tag_scalar_dict={\"train/acc\": train_acc, \"val/acc\": val_acc}, global_step=epoch)\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean cude\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
